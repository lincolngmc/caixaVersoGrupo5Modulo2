{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc88916a",
      "metadata": {
        "id": "dc88916a"
      },
      "source": [
        "# RELATÓRIO DE ANÁLISE — Adult Census Income Dataset (Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184ffa7f",
      "metadata": {
        "id": "184ffa7f"
      },
      "source": [
        "# 1. Visão Geral e Objetivo\n",
        "Instituições financeiras enfrentam o desafio constante de alinhar seus produtos de investimento ao perfil e à capacidade financeira de seus clientes. Nesse cenário, a análise correta de informações socioeconômicas e comportamentais é fundamental para compreender as necessidades e expectativas de cada indivíduo.\n",
        "\n",
        "O presente estudo propõe o desenvolvimento de um modelo preditivo para segmentação de clientes. Para tal, utiliza-se o conjunto de dados *Adult Census Income*, que contém variáveis demográficas essenciais — como idade, escolaridade, ocupação, estado civil e horas de trabalho — simulando as informações tipicamente coletadas em questionários de \"Perfil de Investidor\" (API).\n",
        "\n",
        "A hipótese central é que indivíduos com renda anual superior a 100 mil dólares apresentam padrões socioeconômicos distintos, passíveis de modelagem estatística. Ao identificar com precisão esse perfil, a instituição pode direcionar a oferta de fundos de investimento exclusivos e produtos *premium* de forma proativa.\n",
        "\n",
        "Assim, a pesquisa busca responder: **quais variáveis demográficas mais influenciam a classificação de um indivíduo no grupo de maior renda e como utilizar essas predições para otimizar a estratégia comercial?**\n",
        "\n",
        "A aplicação deste modelo visa impactar diretamente indicadores de negócio, mitigando dois problemas principais:\n",
        "1.  **Oferta Ineficiente:** Evitar o direcionamento de produtos genéricos a clientes sofisticados, otimizando recursos de marketing.\n",
        "2.  **Custo de Oportunidade:** Reduzir a falha na identificação de clientes com alto potencial de aporte, maximizando a Taxa de Conversão e a retenção de carteiras de alto valor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a367f4",
      "metadata": {
        "id": "d7a367f4"
      },
      "source": [
        "# 2. Requisitos Detalhados do Projeto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d1daf3a",
      "metadata": {
        "id": "0d1daf3a"
      },
      "source": [
        "## 2.1. Seleção do Conjunto de Dados (Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3579eb3c",
      "metadata": {
        "id": "3579eb3c"
      },
      "source": [
        "Dataset: Adult Census Income. https://www.kaggle.com/datasets/priyamchoksi/adult-census-income-dataset\n",
        "\n",
        "Conteúdo: Dados extraídos do banco de dados do Censo de 1994 realizado nos Estados Unidos.\n",
        "\n",
        "Tarefa: Verificar se uma pessoa ganha mais de $ 100.000 por ano.\n",
        "\n",
        "Tipo de Atividade: Classificatória\n",
        "\n",
        "Target: \"income\" (renda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8973bb",
      "metadata": {
        "id": "7d8973bb"
      },
      "source": [
        "#### Descrição dos Dados\n",
        "\n",
        "| Feature            | Descrição                                                                                 |\n",
        "| ------------------ | ----------------------------------------------------------------------------------------- |\n",
        "| **age**            | Idade do indivíduo (em anos).                                                             |\n",
        "| **workclass**      | Tipo de vínculo empregatício (ex.: Private, Federal-gov).                                 |\n",
        "| **fnlwgt**         | Peso amostral do censo (*final weight*), indicando quantas pessoas o registro representa. |\n",
        "| **education**      | Nível de escolaridade concluído.                                                          |\n",
        "| **education.num**  | Escolaridade convertida para um valor numérico.                                           |\n",
        "| **marital.status** | Estado civil do indivíduo (ex.: Married-civ-spouse, Divorced).                            |\n",
        "| **occupation**     | Profissão exercida pelo indivíduo.                                                        |\n",
        "| **relationship**   | Relação familiar na residência (ex.: Husband, Not-in-family).                             |\n",
        "| **race**           | Raça/etnia declarada.                                                                     |\n",
        "| **sex**            | Sexo informado.                                                                           |\n",
        "| **capital.gain**   | Ganho anual de capital (ex.: investimentos).                                              |\n",
        "| **capital.loss**   | Perda anual de capital.                                                                   |\n",
        "| **hours.per.week** | Número de horas trabalhadas por semana.                                                   |\n",
        "| **native.country** | País de origem do indivíduo.                                                              |\n",
        "| **income**         | Target: indica se a renda anual é `<=100K` ou `>100K`.                                      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b643e243",
      "metadata": {
        "id": "b643e243"
      },
      "source": [
        "#### Feature: `fnlwgt`\n",
        "\n",
        "O atributo vem do banco de dados do Censo dos EUA de 1994, usado neste Dataset Adult Income Dataset (também chamado de Census Income Dataset).\n",
        "\n",
        "Função:\n",
        "    Cada pessoa registrada no dataset não corresponde apenas a si mesma, mas a um número maior de pessoas com características semelhantes na população.\n",
        "    O fnlwgt é esse número — o peso que o censo atribuiu para garantir que a amostra seja representativa da população.\n",
        "\n",
        "Exemplo prático:\n",
        "\n",
        "    Se um indivíduo tem fnlwgt = 200000, significa que, estatisticamente, ele representa cerca de 200 mil pessoas com perfil semelhante (idade, escolaridade, ocupação etc.).\n",
        "\n",
        "Em suma:\n",
        "- Representa o **peso amostral** atribuído pelo censo a cada registro.  \n",
        "- Indica **quantas pessoas na população real (EUA) possuem características semelhantes** ao indivíduo da amostra.  \n",
        "- Serve para análises estatísticas representativas da população.  \n",
        "- Em tarefas de *machine learning*, geralmente é **descartado no treinamento**, pois não descreve diretamente o indivíduo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 Metodologia e Etapas do Projeto\n",
        "\n",
        "O projeto seguirá o fluxo de trabalho clássico de Machine Learning (ML), dividido nas seguintes etapas principais:\n",
        "\n",
        "#### 1. Pré-processamento e Análise Exploratória (EDA)\n",
        "*   **Tratamento de Missing Values** (Especial atenção aos caracteres \"?\" no dataset).\n",
        "*   **Análise de Viés:** Verificação da distribuição de renda por gênero e raça para entender disparidades históricas nos dados.\n",
        "*   **Codificação de Variáveis** (*Encoding*) e **Escalonamento**.\n",
        "\n",
        "#### 2. Modelagem Preditiva\n",
        "*   **Definição de Baseline:** Criação de um marco zero de performance.\n",
        "*   **Seleção de Algoritmos:** Testes com Regressão Logística (para interpretabilidade) e Ensemble Methods como Random Forest e XGBoost (para performance).\n",
        "*   **Otimização de Hiperparâmetros** para maximizar a generalização.\n",
        "\n",
        "#### 3. Avaliação de Desempenho\n",
        "*   **Foco no Desbalanceamento:** Como a classe \"Alta Renda\" é minoritária, a **Acurácia** será analisada em conjunto com:\n",
        "    *   **Precision & Recall:** Para equilibrar o custo de perder um cliente rico *versus* o custo de ofertar errado.\n",
        "    *   **F1-Score e Curva ROC-AUC:** Para uma visão global da robustez do modelo.\n",
        "\n",
        "#### 4. Interpretabilidade do Modelo\n",
        "*   **Feature Importance:** Identificação das variáveis que mais pesam na decisão.\n",
        "*   **Explicação de Negócio:** Tradução dos pesos matemáticos em *insights* comportamentais para a equipe comercial.\n",
        "\n"
      ],
      "metadata": {
        "id": "lAE1gJSXKgVi"
      },
      "id": "lAE1gJSXKgVi"
    },
    {
      "cell_type": "markdown",
      "id": "c148fd29",
      "metadata": {
        "id": "c148fd29"
      },
      "source": [
        "## 2.2. Definição do Problema de Negócio\n",
        "\n",
        "A pesquisa procura responder: **quais variáveis do perfil de investidor mais influenciam a classificação de clientes como alta renda e como essas informações podem ser utilizadas para otimizar a estratégia de oferta de fundos de investimento?**\n",
        "\n",
        "O presente estudo propõe a utilização do **Machine Learning (ML)** aplicado a dados socioeconômicos (simulados pelo *Adult Census Income Dataset*) para criar uma **ferramenta preditiva** de classificação de clientes.\n",
        "\n",
        "#### Objetivo Central\n",
        "\n",
        "Construir um **Modelo de Classificação Binária** capaz de prever, com alta acurácia, quais clientes pertencem ao segmento de **Alta Renda** (renda anual simulada **acima de $100.000**), com base em variáveis de perfil (idade, educação, ocupação, etc.).\n",
        "\n",
        "#### Hipótese Central\n",
        "\n",
        "Variáveis demográficas e de emprego coletadas no \"Questionário de Perfil\" do cliente possuem **padrões estatisticamente significativos** que diferenciam os clientes de alta renda, e o ML é a ferramenta ideal para capturar essa não-linearidade.\n",
        "\n",
        "#### Impacto Estratégico e Valor Agregado\n",
        "\n",
        "A implantação de um modelo preditivo como este em ambiente de produção oferece vantagens estratégicas imediatas:\n",
        "\n",
        "* **Otimização de *Cross-Selling***: Permite que a área comercial direcione de forma **proativa e personalizada** a oferta de **Fundos de Investimento Exclusivos, Previdência Privada e *Wealth Management*** apenas para o grupo de clientes com maior probabilidade de conversão.\n",
        "* **Melhoria do *Customer Experience***: Reduz a frustração do cliente ao garantir que as recomendações de investimento sejam altamente **relevantes e alinhadas** com seu potencial financeiro.\n",
        "* **Base para *Credit Scoring***: A técnica de modelagem de classificação e a análise das **variáveis preditivas mais importantes** (*feature importance*) fornecem *insights* valiosos que podem ser estendidos para outros modelos críticos do banco, como risco de crédito e detecção de fraude.\n",
        "\n",
        "**Em suma:** Este projeto demonstra a capacidade de transformar dados estáticos de questionário em **inteligência de negócios acionável**, posicionando a instituição na vanguarda da **tomada de decisão orientada por dados** no mercado financeiro.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4668bf4f",
      "metadata": {
        "id": "4668bf4f"
      },
      "source": [
        "## 2.3 Pré-processamento e Engenharia de Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d549d3",
      "metadata": {
        "id": "28d549d3"
      },
      "source": [
        "#### Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67505c7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67505c7f",
        "outputId": "20fc23cf-bce7-4d14-d742-6668e58d333d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy matplotlib seaborn scikit-learn statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da7b7541",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "da7b7541",
        "outputId": "b52eb242-c277-4928-c7cb-38a7bf8be182"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
              "0   90         ?   77053       HS-grad              9        Widowed   \n",
              "1   29   Private  242597     Bachelors             13  Never-married   \n",
              "2   35   Private   96824  Some-college             10       Divorced   \n",
              "3   19         ?  234519  Some-college             10  Never-married   \n",
              "4   22   Private   71379  Some-college             10  Never-married   \n",
              "\n",
              "       occupation   relationship   race     sex  capital.gain  capital.loss  \\\n",
              "0               ?  Not-in-family  White  Female             0          4356   \n",
              "1    Tech-support  Not-in-family  White    Male             0             0   \n",
              "2   Other-service      Unmarried  White  Female             0             0   \n",
              "3               ?      Own-child  White    Male             0             0   \n",
              "4  Prof-specialty  Not-in-family  White  Female             0             0   \n",
              "\n",
              "   hours.per.week native.country  income  \n",
              "0              40  United-States  <=100K  \n",
              "1              40  United-States  <=100K  \n",
              "2              40  United-States  <=100K  \n",
              "3              35  United-States  <=100K  \n",
              "4              35  United-States  <=100K  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e858e727-3bf0-4a14-b784-cde761eeb0de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education.num</th>\n",
              "      <th>marital.status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital.gain</th>\n",
              "      <th>capital.loss</th>\n",
              "      <th>hours.per.week</th>\n",
              "      <th>native.country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>?</td>\n",
              "      <td>77053</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=100K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>Private</td>\n",
              "      <td>242597</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Tech-support</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=100K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35</td>\n",
              "      <td>Private</td>\n",
              "      <td>96824</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=100K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>?</td>\n",
              "      <td>234519</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>?</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=100K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22</td>\n",
              "      <td>Private</td>\n",
              "      <td>71379</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=100K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e858e727-3bf0-4a14-b784-cde761eeb0de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e858e727-3bf0-4a14-b784-cde761eeb0de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e858e727-3bf0-4a14-b784-cde761eeb0de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9db55220-95df-44a3-ba90-09344d44ec5e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9db55220-95df-44a3-ba90-09344d44ec5e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9db55220-95df-44a3-ba90-09344d44ec5e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 19161,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 17,\n        \"max\": 90,\n        \"num_unique_values\": 73,\n        \"samples\": [\n          22,\n          67,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"workclass\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Without-pay\",\n          \"Private\",\n          \"Local-gov\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fnlwgt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 106795,\n        \"min\": 12285,\n        \"max\": 1484705,\n        \"num_unique_values\": 14895,\n        \"samples\": [\n          158077,\n          157886,\n          165278\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"HS-grad\",\n          \"Bachelors\",\n          \"9th\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education.num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 16,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          9,\n          13,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital.status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Widowed\",\n          \"Never-married\",\n          \"Married-spouse-absent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"occupation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Craft-repair\",\n          \"Protective-serv\",\n          \"?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relationship\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Not-in-family\",\n          \"Unmarried\",\n          \"Wife\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"race\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Other\",\n          \"Asian-Pac-Islander\",\n          \"Black\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Male\",\n          \"Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capital.gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1077,\n        \"min\": 0,\n        \"max\": 41310,\n        \"num_unique_values\": 63,\n        \"samples\": [\n          34095,\n          3273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"capital.loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 351,\n        \"min\": 0,\n        \"max\": 4356,\n        \"num_unique_values\": 73,\n        \"samples\": [\n          1816,\n          1669\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hours.per.week\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 99,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          66,\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"native.country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"Ecuador\",\n          \"Guatemala\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"<=1\",\n          \"<=100K\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/adult_census_income.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "13a90f84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13a90f84",
        "outputId": "26ca0392-c0bc-4ecb-fa7a-d38fccd517af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32561 entries, 0 to 32560\n",
            "Data columns (total 1 columns):\n",
            " #   Column                                                                                                                                                       Non-Null Count  Dtype \n",
            "---  ------                                                                                                                                                       --------------  ----- \n",
            " 0   age;workclass;fnlwgt;education;education.num;marital.status;occupation;relationship;race;sex;capital.gain;capital.loss;hours.per.week;native.country;income  32561 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 254.5+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f51ed5",
      "metadata": {
        "id": "87f51ed5"
      },
      "source": [
        "Total de Registros: 32.561\n",
        "\n",
        "Estrutura:\n",
        "\n",
        "| #  | Column          | Non-Null Count | Dtype  |\n",
        "|----|-----------------|----------------|--------|\n",
        "| 0  | age             | 32561 non-null | int64  |\n",
        "| 1  | workclass       | 32561 non-null | object |\n",
        "| 2  | education       | 32561 non-null | object |\n",
        "| 3  | education.num   | 32561 non-null | int64  |\n",
        "| 4  | marital.status  | 32561 non-null | object |\n",
        "| 5  | occupation      | 32561 non-null | object |\n",
        "| 6  | relationship    | 32561 non-null | object |\n",
        "| 7  | race            | 32561 non-null | object |\n",
        "| 8  | sex             | 32561 non-null | object |\n",
        "| 9  | capital.gain    | 32561 non-null | int64  |\n",
        "| 10 | capital.loss    | 32561 non-null | int64  |\n",
        "| 11 | hours.per.week  | 32561 non-null | int64  |\n",
        "| 12 | native.country  | 32561 non-null | object |\n",
        "| 13 | income          | 32561 non-null | object |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13b87af",
      "metadata": {
        "id": "c13b87af"
      },
      "source": [
        "#### Tratamento de Valores Nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a22d1c",
      "metadata": {
        "id": "19a22d1c"
      },
      "source": [
        "Há campos de texto com valores <b>\"?\"</b> que devem ser tratados como <b>Nulos</b>. Neste caso, precisamos substituir todos  eles por <i>Null</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dfbfd79e",
      "metadata": {
        "id": "dfbfd79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "fd3054b3-a5fd-45fc-a7bb-5891bfa72dd2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'workclass'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'workclass'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1467447205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'workclass'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'workclass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'occupation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'occupation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'native.country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'native.country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'United-States'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'workclass'"
          ]
        }
      ],
      "source": [
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "\n",
        "# Agora substitui o \"?\" limpo\n",
        "df = df.replace(\"\", np.nan) # Replacing empty strings which might result from stripping\n",
        "df = df.replace(\"?\", np.nan)\n",
        "\n",
        "df['workclass'] = df['workclass'].fillna('other')\n",
        "df['occupation'] = df['occupation'].fillna('other')\n",
        "df['native.country'] = df['native.country'].fillna('United-States')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40df2ae4",
      "metadata": {
        "id": "40df2ae4"
      },
      "source": [
        "Visualização dos dados tratados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdc75da",
      "metadata": {
        "id": "4fdc75da"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ea3d5c",
      "metadata": {
        "id": "68ea3d5c"
      },
      "source": [
        "Verificando se há valores nulos que precisam ser tratados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54e0bd2",
      "metadata": {
        "id": "c54e0bd2"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0475da2a",
      "metadata": {
        "id": "0475da2a"
      },
      "source": [
        "Notamos que as features <b>workclass</b>, <b>occupation</b> e <b>native.country</b> contém valores nulos. Tratamentos:\n",
        " - <i>workclass</i>: é o tipo de contrato de tabalho, vamos substituir nulos por \"outros\";\n",
        " - <i>occupation</i>: é o cargo exercido. Como pode ser que a pessoa não esteja trabalhando, vamos também substituir por \"outros\";\n",
        " - <i>native.country</i>: país de origem da pessoa. Podemos deixar como \"outros\" mas, levando em consideração que <b>United-States</b> representa mais de 90%, podemos substituir por \"United-States\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8598e2b",
      "metadata": {
        "id": "e8598e2b"
      },
      "outputs": [],
      "source": [
        "df['native.country'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A coluna occupation já possui uma categoria chamada Other-service. Para não misturar, usar o termo \"Desconhecido\" ou \"Nao-Informado\" em vez de apenas \"Outros\"."
      ],
      "metadata": {
        "id": "G3np2a1NNPUi"
      },
      "id": "G3np2a1NNPUi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1613feec",
      "metadata": {
        "id": "1613feec"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b99e2d2",
      "metadata": {
        "id": "9b99e2d2"
      },
      "source": [
        "Vamos agora remover a feature <b>fnlwgt</b> por não ser relevante para o projeto. Conforme vimos no início, trata-se de uma variável estatística inserida pelo Censu de modo a refletir as características da população real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a9aa35",
      "metadata": {
        "id": "68a9aa35"
      },
      "outputs": [],
      "source": [
        "df = df.drop('fnlwgt', axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809f78e2",
      "metadata": {
        "id": "809f78e2"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Vamos agora verificar se a feature <b>age (idade)</b> possui algum valor anormal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9811bf78",
      "metadata": {
        "id": "9811bf78"
      },
      "outputs": [],
      "source": [
        "print(\"Idade mínima:\", df[\"age\"].min())\n",
        "print(\"Idade máxima:\", df[\"age\"].max())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Visualização Gráfica (Para ver a distribuição e outliers)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Boxplot: Os pontos fora dos \"bigodes\" são estatisticamente outliers\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=df['age'])\n",
        "plt.title('Boxplot da Idade')\n",
        "\n",
        "# Histograma: Para ver a concentração\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['age'], bins=20, kde=True)\n",
        "plt.title('Distribuição de Idade')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V8XywIPtSIli"
      },
      "id": "V8XywIPtSIli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com base nos gráficos gerados:\n",
        "\n",
        "**1. Análise da Distribuição (Histograma - Direita)**\n",
        "*   **Assimetria à Direita (Right-Skewed):** A curva mostra claramente uma assimetria positiva. A grande massa de dados (o pico) está concentrada em indivíduos mais jovens, especificamente entre **20 e 40 anos**.\n",
        "*   **Força de Trabalho:** Isso reflete a realidade demográfica do mercado de trabalho: há muito mais pessoas no início e meio de carreira do que no final (pós-aposentadoria).\n",
        "*   **Cauda Longa:** A curva desce suavemente até os 90 anos, confirmando que há dados representativos de todas as faixas etárias, mas com menor frequência nas idades avançadas.\n",
        "\n",
        "**2. Análise de Outliers e Quartis (Boxplot - Esquerda)**\n",
        "*   **Mediana e Quartis:**\n",
        "    *   A linha central do boxplot (Mediana) está em aproximadamente **37 anos**.\n",
        "    *   50% das pessoas (a caixa azul) têm entre ~28 (1º quartil) e ~48 anos (3º quartil).\n",
        "*   **Os \"Outliers\":**\n",
        "    *   Observe os pontos pretos (círculos) à direita do gráfico. Eles começam a aparecer por volta dos **78 anos** e vão até **90 anos**.\n",
        "    *   Estatisticamente, eles são *outliers* (pois estão distantes da média).\n",
        "\n",
        "**Resumo para o relatório:**\n",
        "> \"A variável `age` apresenta uma distribuição assimétrica à direita, com mediana de 37 anos. Embora o Boxplot indique a presença de outliers estatísticos acima de 78 anos, optou-se por **mantê-los**, pois representam uma parcela válida da população (idosos) que é estrategicamente relevante para a oferta de produtos de investimento e gestão de patrimônio.\""
      ],
      "metadata": {
        "id": "X-aevpnfStkC"
      },
      "id": "X-aevpnfStkC"
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "EQcaF31bTPVK"
      },
      "id": "EQcaF31bTPVK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar quantas linhas são duplicatas exatas\n",
        "duplicatas = df.duplicated().sum()\n",
        "print(f\"Quantidade de linhas duplicadas: {duplicatas}\")\n",
        "\n",
        "# Remover duplicatas (Mantenha a primeira ocorrência)\n",
        "if duplicatas > 0:\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    print(\"✅ Duplicatas removidas com sucesso!\")\n",
        "    print(f\"Novo shape do dataset: {df.shape}\")"
      ],
      "metadata": {
        "id": "DmvcJj8ITsTq"
      },
      "id": "DmvcJj8ITsTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3e29c9b8",
      "metadata": {
        "id": "3e29c9b8"
      },
      "source": [
        "### Análise Exploratória"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c825d374",
      "metadata": {
        "id": "c825d374"
      },
      "source": [
        "Descrição Estatística (apenas colunas numéricas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef650cfb",
      "metadata": {
        "id": "ef650cfb"
      },
      "outputs": [],
      "source": [
        "colunas_numericas = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
        "df[colunas_numericas].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5f222d",
      "metadata": {
        "id": "fd5f222d"
      },
      "source": [
        "Distribuições das variáveis numéricas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf2f3c4",
      "metadata": {
        "id": "3cf2f3c4"
      },
      "outputs": [],
      "source": [
        "# Distribuições das variáveis numéricas\n",
        "colunas_numericas = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
        "\n",
        "n_cols = len(colunas_numericas)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_cols, figsize=(16, 5))\n",
        "\n",
        "# Se houver apenas 1 coluna, axes não será um array\n",
        "if n_cols == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, col in zip(axes, colunas_numericas):\n",
        "    sns.histplot(df[col].dropna(), kde=True, ax=ax, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(f'{col.upper()}', fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.set_xlabel('Valor', fontsize=11)\n",
        "    ax.set_ylabel('Frequência', fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Distribuição das Variáveis Numéricas', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feec029",
      "metadata": {
        "id": "0feec029"
      },
      "source": [
        "Analise Boxplot das features numéricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dac23c9",
      "metadata": {
        "id": "4dac23c9"
      },
      "outputs": [],
      "source": [
        "# Boxplots das numéricas por target\n",
        "colunas_numericas = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
        "\n",
        "n_cols = len(colunas_numericas)\n",
        "\n",
        "fig, axes = plt.subplots(1, n_cols, figsize=(16, 5))\n",
        "\n",
        "# Se houver apenas 1 coluna, axes não será um array\n",
        "if n_cols == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, col in zip(axes, colunas_numericas):\n",
        "    sns.boxplot(x='income', y=col, data=df, hue='income', ax=ax, palette='Set2', legend=False)\n",
        "    ax.set_title(f'{col.upper()} por Income', fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.set_xlabel('Income Category', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Valor', fontsize=11, fontweight='bold')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Distribuição das Variáveis Numéricas por Categoria de Renda', fontsize=14, fontweight='bold', y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "085480b9",
      "metadata": {
        "id": "085480b9"
      },
      "source": [
        "Correlação e Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1521fdb8",
      "metadata": {
        "id": "1521fdb8"
      },
      "outputs": [],
      "source": [
        "# Selecionar colunas numéricas\n",
        "colunas_numericas = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[colunas_numericas].corr(), annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Correlação (numéricas)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd3b68c",
      "metadata": {
        "id": "5dd3b68c"
      },
      "source": [
        "Composição das Categorias e sua relação com o Target:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fd6e08",
      "metadata": {
        "id": "e1fd6e08"
      },
      "outputs": [],
      "source": [
        "# Lista de colunas categóricas\n",
        "colunas_categoricas = ['workclass', 'education', 'marital.status', 'occupation',\n",
        "            'relationship', 'race', 'sex', 'native.country']\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 18))  # Ajuste tamanho\n",
        "\n",
        "for ax, col in zip(axes.flat, colunas_categoricas):\n",
        "    if col in df.columns:\n",
        "        # Agrupar por categoria e income\n",
        "        counts = df.groupby([col, 'income']).size().unstack(fill_value=0)\n",
        "\n",
        "        # Selecionar top 8 categorias mais frequentes\n",
        "        top_categories = df[col].value_counts().head(8).index\n",
        "        counts = counts.loc[top_categories]\n",
        "\n",
        "        # Plotar barras empilhadas (stacked)\n",
        "        counts.plot(kind='barh', stacked=True, ax=ax,\n",
        "                    color=['steelblue', 'orange'], alpha=0.8)\n",
        "\n",
        "        ax.set_title(f'{col} vs Income', fontsize=11, fontweight='bold', pad=10)\n",
        "        ax.set_xlabel('Count', fontsize=10)\n",
        "        ax.tick_params(axis='y', labelsize=9)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cffb981d",
      "metadata": {
        "id": "cffb981d"
      },
      "source": [
        "Análise do Target (Distribuição da Renda):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f627b025",
      "metadata": {
        "id": "f627b025"
      },
      "outputs": [],
      "source": [
        "# Balanceamento da target\n",
        "counts = df['income'].value_counts()\n",
        "pct = df['income'].value_counts(normalize=True) * 100\n",
        "print(\"Target counts:\\n\", counts)\n",
        "print(\"\\nTarget percent:\\n\", pct.round(2))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "bars = ax.bar(counts.index, counts.values, color=['#1f77b4', '#ff7f0e'])\n",
        "\n",
        "# Adicionar rótulos percentuais no topo de cada coluna\n",
        "for i, (bar, percentage) in enumerate(zip(bars, pct.values)):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{percentage:.1f}%',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_title('Target distribution')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_xlabel('Income')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140c8af5",
      "metadata": {
        "id": "140c8af5"
      },
      "source": [
        "#### Cálculo do VIF (Variance Inflation Factor)\n",
        "\n",
        "O VIF é uma métrica usada para detectar multicolinearidade — ou seja, quando uma variável explicativa pode ser explicada linearmente pelas outras variáveis, o que atrapalha modelos estatísticos/regressões.\n",
        "\n",
        "Para cada coluna, calcula o quanto ela é explicada pelas demais.\n",
        "\n",
        "- VIF ≈ 1 → sem multicolinearidade\n",
        "\n",
        "- VIF > 5 → possível problema\n",
        "\n",
        "- VIF > 10 → multicolinearidade severa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169d1655",
      "metadata": {
        "id": "169d1655"
      },
      "outputs": [],
      "source": [
        "# VIF rápido nas numéricas (ajuda decidir remoções)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if len(num_cols) >= 2:\n",
        "    X_num = df[num_cols].dropna()\n",
        "    X_num_const = sm.add_constant(X_num)\n",
        "    vif_data = pd.DataFrame({\n",
        "        'feature': X_num.columns,\n",
        "        'VIF': [variance_inflation_factor(X_num_const.values, i+1) for i in range(len(X_num.columns))]\n",
        "    }).sort_values('VIF', ascending=False)\n",
        "    print(\"\\nVIF (numéricas):\\n\", vif_data.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32767437",
      "metadata": {
        "id": "32767437"
      },
      "source": [
        "A análise do VIF nos mostra que todas as variáveis numéricas são independentes e que não há necessidade de remoção."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc5eb59d",
      "metadata": {
        "id": "dc5eb59d"
      },
      "source": [
        "### Tratamento, Conversão e Criação de Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55f60dd",
      "metadata": {
        "id": "a55f60dd"
      },
      "source": [
        "##### Convertendo Classificação Textual para Numérica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca4b3fa",
      "metadata": {
        "id": "6ca4b3fa"
      },
      "source": [
        "A feature <b>education</b> é categórica e no dataset ela vem tanto em formato object quanto em int64 (<b>education.num</b>), ambas trazendo a mesma informação em formatos diferentes.\n",
        "Podemos fazer o mesmo com as demais colunas textuais categóricas.\n",
        "Existe uma biblioteca que faz isso de forma automática chamada <b>LabelEncoder</b>.\n",
        "\n",
        "O mapeamento do LabelEncoder não muda aleatoriamente a cada execução. Ele segue uma regra fixa:\n",
        "\n",
        "    - As classes são ordenadas em ordem alfabética (lexicográfica).\n",
        "\n",
        "    - Cada categoria recebe um número inteiro começando em 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090b8e69",
      "metadata": {
        "id": "090b8e69"
      },
      "source": [
        "Preferimos fazer este processo de forma manual, assim garantimos mais transparencia e facilita nossa visualização e futuras consultas. Caso haja nova categoria, basta acrescentar no mapeamento abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98c9c91",
      "metadata": {
        "id": "b98c9c91"
      },
      "outputs": [],
      "source": [
        "# Definir mapeamentos fixos\n",
        "mappings = {\n",
        "    \"workclass\": {\n",
        "        \"Federal-gov\": 0, \"Local-gov\": 1, \"Never-worked\": 2, \"Private\": 3,\n",
        "        \"Self-emp-inc\": 4, \"Self-emp-not-inc\": 5, \"State-gov\": 6,\n",
        "        \"Without-pay\": 7, \"other\": 8\n",
        "    },\n",
        "    \"marital.status\": {\n",
        "        \"Divorced\": 0, \"Married-AF-spouse\": 1, \"Married-civ-spouse\": 2,\n",
        "        \"Married-spouse-absent\": 3, \"Never-married\": 4,\n",
        "        \"Separated\": 5, \"Widowed\": 6\n",
        "    },\n",
        "    \"occupation\": {\n",
        "        \"Adm-clerical\": 0, \"Armed-Forces\": 1, \"Craft-repair\": 2,\n",
        "        \"Exec-managerial\": 3, \"Farming-fishing\": 4, \"Handlers-cleaners\": 5,\n",
        "        \"Machine-op-inspct\": 6, \"Other-service\": 7, \"Priv-house-serv\": 8,\n",
        "        \"Prof-specialty\": 9, \"Protective-serv\": 10, \"Sales\": 11,\n",
        "        \"Tech-support\": 12, \"Transport-moving\": 13, \"other\": 14\n",
        "    },\n",
        "    \"relationship\": {\n",
        "        \"Husband\": 0, \"Not-in-family\": 1, \"Other-relative\": 2,\n",
        "        \"Own-child\": 3, \"Unmarried\": 4, \"Wife\": 5\n",
        "    },\n",
        "    \"race\": {\n",
        "        \"Amer-Indian-Eskimo\": 0, \"Asian-Pac-Islander\": 1,\n",
        "        \"Black\": 2, \"Other\": 3, \"White\": 4\n",
        "    },\n",
        "    \"sex\": {\n",
        "        \"Female\": 0, \"Male\": 1\n",
        "    },\n",
        "    \"native.country\": {\n",
        "        \"Cambodia\": 0, \"Canada\": 1, \"China\": 2, \"Columbia\": 3, \"Cuba\": 4,\n",
        "        \"Dominican-Republic\": 5, \"Ecuador\": 6, \"El-Salvador\": 7, \"England\": 8,\n",
        "        \"France\": 9, \"Germany\": 10, \"Greece\": 11, \"Guatemala\": 12, \"Haiti\": 13,\n",
        "        \"Holand-Netherlands\": 14, \"Honduras\": 15, \"Hong\": 16, \"Hungary\": 17,\n",
        "        \"India\": 18, \"Iran\": 19, \"Ireland\": 20, \"Italy\": 21, \"Jamaica\": 22,\n",
        "        \"Japan\": 23, \"Laos\": 24, \"Mexico\": 25, \"Nicaragua\": 26,\n",
        "        \"Outlying-US(Guam-USVI-etc)\": 27, \"Peru\": 28, \"Philippines\": 29,\n",
        "        \"Poland\": 30, \"Portugal\": 31, \"Puerto-Rico\": 32, \"Scotland\": 33,\n",
        "        \"South\": 34, \"Taiwan\": 35, \"Thailand\": 36, \"Trinadad&Tobago\": 37,\n",
        "        \"United-States\": 38, \"Vietnam\": 39, \"Yugoslavia\": 40, \"other\": 41\n",
        "    },\n",
        "    \"income\": {\n",
        "        \"<=100K\": 0, \">100K\": 1\n",
        "    }\n",
        "}\n",
        "\n",
        "# Função para aplicar os mapeamentos\n",
        "def apply_mappings(df, mappings):\n",
        "    for col, mapping in mappings.items():\n",
        "        df[col + \".num\"] = df[col].map(mapping)\n",
        "    return df\n",
        "\n",
        "# Exemplo de uso\n",
        "df = apply_mappings(df, mappings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b0f225",
      "metadata": {
        "id": "73b0f225"
      },
      "outputs": [],
      "source": [
        "# Tabela de correspondência entre native.country e native.country.num\n",
        "df[[\"native.country\", \"native.country.num\"]].drop_duplicates().sort_values(\"native.country.num\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c2109c",
      "metadata": {
        "id": "09c2109c"
      },
      "source": [
        "#### Normalizando Escala das Features Capital.Gain e Capital.Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbe0332",
      "metadata": {
        "id": "4bbe0332"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Selecionar colunas que deseja normalizar\n",
        "colunas_capital = [\"capital.gain\", \"capital.loss\"]\n",
        "\n",
        "# Instanciar o scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Aplicar o scaler e criar novas colunas com sufixo \".num\"\n",
        "df[[col + \".num\" for col in colunas_capital]] = scaler.fit_transform(df[colunas_capital])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "177421df",
      "metadata": {
        "id": "177421df"
      },
      "source": [
        "#### Testando as Features Categóricas Numéricas usando do PCA (Análise de Componentes Principais)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3768076b",
      "metadata": {
        "id": "3768076b"
      },
      "outputs": [],
      "source": [
        "# Selecionar colunas numéricas\n",
        "colunas_numericas = ['age', 'education.num', 'workclass.num', 'marital.status.num', 'occupation.num', 'relationship.num', 'race.num', 'sex.num', 'native.country.num', 'capital.gain.num', 'capital.loss.num', 'hours.per.week']\n",
        "df_num = df[colunas_numericas]\n",
        "df_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53beb81f",
      "metadata": {
        "id": "53beb81f"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_num)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "df_pca = pd.DataFrame({\n",
        "    \"PC1\": pca_result[:, 0],\n",
        "    \"PC2\": pca_result[:, 1],\n",
        "    \"income\": df[\"income\"]\n",
        "})\n",
        "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\", hue=\"income\", alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfba954b",
      "metadata": {
        "id": "bfba954b"
      },
      "source": [
        "#### Criando novas Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46931445",
      "metadata": {
        "id": "46931445"
      },
      "source": [
        "O PCA nos mostrou que as features atuais aparentemente são suficientes para nos entregar modelos com bons resultados. O gráfico nos mostra uma clara segmentação dos targets.\n",
        "\n",
        "No entanto, vamos criar novas Features e verificar se o uso delas trarão alguma melhoria.\n",
        "\n",
        "Iremos criar as 4 features abaixo:\n",
        "\n",
        "- <b>capital.result.num</b>: Será o resultado da diferença entre capital.gain e capital.loss, em outras palavras, será o valor liquido de ganho de capital anual.\n",
        "\n",
        "- <b>education.group.num</b>: Será uma redução na quantidade de níveis de educação de 17 categorias para 3 categorias conforme a seguir:\n",
        "\n",
        "| Categoria Original                                     | education.num | Novo Grupo       | Novo Grupo Num |\n",
        "|--------------------------------------------------------|---------------|------------------|----------------|\n",
        "| Preschool até 12th                                     | 1–8           | ≤ High School    |        1       |\n",
        "| HS-grad, Some-college, Assoc-voc, Assoc-acdm, Bachelors| 9–13          | Undergraduate    |        2       |\n",
        "| Masters, Prof-school, Doctorate                        | 14–16         | Postgraduate     |        3       |\n",
        "\n",
        "\n",
        "- <b>workclass.group.num</b>: Será redução na quantidade de tipos de vínculo empregatício de 9 para 3, conforme a seguir:\n",
        "\n",
        "| Categoria Original                          | workclass.num | Novo Grupo (workclass_group) | Grupo Nº |\n",
        "|---------------------------------------------|---------------|------------------------------|----------|\n",
        "| Federal-gov, Local-gov, State-gov, Private  | 0, 1, 3, 6    | Employed                     | 1        |\n",
        "| Self-emp-inc, Self-emp-not-inc              | 4, 5          | Self-employed                | 2        |\n",
        "| Never-worked, Without-pay, other            | 2, 7, 8       | Not Working/Other            | 3        |\n",
        "\n",
        "- <b>age.group.num</b>: Criar faixa etária '18-30', '31-45', '46-60', '60+'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05ac5a3",
      "metadata": {
        "id": "a05ac5a3"
      },
      "source": [
        "Criando Capital.Result.Num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb474ef",
      "metadata": {
        "id": "afb474ef"
      },
      "outputs": [],
      "source": [
        "df['capital.result'] = df['capital.gain'] - df['capital.loss']\n",
        "\n",
        "# Instanciar o scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Aplicar o scaler e criar novas colunas com sufixo \".num\"\n",
        "df['capital.result.num'] = scaler.fit_transform(df[['capital.result']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cd3391",
      "metadata": {
        "id": "12cd3391"
      },
      "source": [
        "Criando age.group.num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8364d28",
      "metadata": {
        "id": "b8364d28"
      },
      "outputs": [],
      "source": [
        "# Criar faixas de idade\n",
        "df['age.group'] = pd.cut(df['age'], bins=[16, 30, 45, 60, 100],\n",
        "                         labels=['17-30', '31-45', '46-60', '60+'])\n",
        "\n",
        "def segmenta_idade(x):\n",
        "    # Grupo 1: Jovens adultos\n",
        "    if x == '17-30':\n",
        "        return 1\n",
        "    # Grupo 2: Adultos em fase intermediária\n",
        "    elif x == '31-45':\n",
        "        return 2\n",
        "    # Grupo 3: Adultos maduros\n",
        "    elif x == '46-60':\n",
        "        return 3\n",
        "    # Grupo 4: Idosos\n",
        "    else:\n",
        "        return 4\n",
        "\n",
        "# Aplicando ao dataframe\n",
        "df['age.group.num'] = df['age.group'].apply(segmenta_idade)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461670d8",
      "metadata": {
        "id": "461670d8"
      },
      "source": [
        "Criando education.group.num e workclass.group.num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba45085",
      "metadata": {
        "id": "eba45085"
      },
      "outputs": [],
      "source": [
        "def simplifica_num_escolaridade(x):\n",
        "    # Grupo 1: até ensino médio incompleto\n",
        "    if x in ['Preschool','1st-4th','5th-6th','7th-8th','9th','10th','11th','12th','HS-grad']:\n",
        "        return 1\n",
        "    # Grupo 2: graduação incompleta ou completa\n",
        "    elif x in ['Some-college','Assoc-voc','Assoc-acdm','Bachelors']:\n",
        "        return 2\n",
        "    # Grupo 3: pós-graduação\n",
        "    else:  # Masters, Prof-school, Doctorate\n",
        "        return 3\n",
        "\n",
        "df['education.group.num'] = df['education'].apply(simplifica_num_escolaridade)\n",
        "\n",
        "\n",
        "def simplifica_num_vinculo_empregaticio(x):\n",
        "    # Grupo 1: empregados (governo ou setor privado)\n",
        "    if x in ['Federal-gov','Local-gov','State-gov','Private']:\n",
        "        return 1\n",
        "    # Grupo 2: autônomos\n",
        "    elif x in ['Self-emp-inc','Self-emp-not-inc']:\n",
        "        return 2\n",
        "    # Grupo 3: não empregados ou outros\n",
        "    else:  # Never-worked, Without-pay, other\n",
        "        return 3\n",
        "\n",
        "df['workclass.group.num'] = df['workclass'].apply(simplifica_num_vinculo_empregaticio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca1ecfc",
      "metadata": {
        "id": "8ca1ecfc"
      },
      "outputs": [],
      "source": [
        "colunas_numericas_new = ['marital.status.num', 'occupation.num', 'relationship.num', 'race.num',\n",
        "                  'sex.num', 'native.country.num', 'capital.result.num', 'age.group.num',\n",
        "                  'education.group.num', 'workclass.group.num'\n",
        "                ]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "df_num_new.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07ad063",
      "metadata": {
        "id": "b07ad063"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- PCA com variáveis originais ---\n",
        "colunas_numericas_original = [\n",
        "    'age', 'education.num', 'workclass.num', 'marital.status.num', 'occupation.num',\n",
        "    'relationship.num', 'race.num', 'sex.num', 'native.country.num',\n",
        "    'capital.gain.num', 'capital.loss.num', 'hours.per.week'\n",
        "]\n",
        "\n",
        "df_num_original = df[colunas_numericas_original]\n",
        "\n",
        "scaler_original = StandardScaler()\n",
        "scaled_data_original = scaler_original.fit_transform(df_num_original)\n",
        "\n",
        "pca_original = PCA(n_components=2)\n",
        "pca_result_original = pca_original.fit_transform(scaled_data_original)\n",
        "\n",
        "df_pca_original = pd.DataFrame({\n",
        "    \"PC1\": pca_result_original[:, 0],\n",
        "    \"PC2\": pca_result_original[:, 1],\n",
        "    \"income\": df[\"income\"]\n",
        "})\n",
        "\n",
        "# --- PCA com variáveis novas ---\n",
        "colunas_numericas_new = ['marital.status.num', 'occupation.num', 'relationship.num', 'race.num',\n",
        "                  'sex.num', 'native.country.num', 'capital.result.num', 'age.group.num',\n",
        "                  'education.group.num', 'workclass.group.num'\n",
        "                ]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "\n",
        "scaler_new = StandardScaler()\n",
        "scaled_data_new = scaler_new.fit_transform(df_num_new)\n",
        "\n",
        "pca_new = PCA(n_components=2)\n",
        "pca_result_new = pca_new.fit_transform(scaled_data_new)\n",
        "\n",
        "df_pca_new = pd.DataFrame({\n",
        "    \"PC1\": pca_result_new[:, 0],\n",
        "    \"PC2\": pca_result_new[:, 1],\n",
        "    \"income\": df[\"income\"]\n",
        "})\n",
        "\n",
        "# --- Plotar lado a lado ---\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "sns.scatterplot(data=df_pca_original, x=\"PC1\", y=\"PC2\", hue=\"income\", alpha=0.7, ax=axes[0])\n",
        "axes[0].set_title(\"PCA - Variáveis Originais\")\n",
        "\n",
        "sns.scatterplot(data=df_pca_new, x=\"PC1\", y=\"PC2\", hue=\"income\", alpha=0.7, ax=axes[1])\n",
        "axes[1].set_title(\"PCA - Variáveis Novas\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8b25da",
      "metadata": {
        "id": "5e8b25da"
      },
      "source": [
        "As novas features trouxeram mais informação ao PCA:\n",
        "\n",
        "1. Os clusters ficaram muito mais separados verticalmente.\n",
        "\n",
        "2. O cluster superior ficou mais compacto e mais estreito horizontalmente e o cluster inferior ficou mais concentrado e menos espalhado.\n",
        "\n",
        "3. A distribuição horizontal (PC1) ficou mais assimétrica. A classe >50K (laranja) está muito mais deslocada para a direita. Isso sugere que uma ou mais das novas features carregam informação que separa parcialmente essa classe.\n",
        "\n",
        "4. A separação entre as classes ficou ligeiramente mais perceptível. Os pontos laranja estão menos misturados no cluster central.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Análise da Matriz de Correlação e Multicolinearidade"
      ],
      "metadata": {
        "id": "guuuqieEFzrF"
      },
      "id": "guuuqieEFzrF"
    },
    {
      "cell_type": "code",
      "source": [
        "colunas_para_correlacao = [\n",
        "    'age', 'hours.per.week',\n",
        "    'education.num', 'workclass.num', 'marital.status.num', 'occupation.num',\n",
        "    'relationship.num', 'race.num', 'sex.num', 'native.country.num',\n",
        "    'capital.gain.num', 'capital.loss.num',\n",
        "    'capital.result.num', 'age.group.num', 'education.group.num', 'workclass.group.num',\n",
        "    'income.num'\n",
        "]\n",
        "\n",
        "# Selecionar as colunas do DataFrame\n",
        "df_correlacao = df[colunas_para_correlacao]\n",
        "\n",
        "# Calcular a matriz de correlação\n",
        "matriz_correlacao = df_correlacao.corr()\n",
        "\n",
        "# Plotar o heatmap\n",
        "plt.figure(figsize=(14, 12)) # Adjusted figure size for more variables\n",
        "sns.heatmap(matriz_correlacao, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title('Matriz de Correlação de Todas as Variáveis do Modelo', fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DWNDiV4FGcnT"
      },
      "id": "DWNDiV4FGcnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f9c95c"
      },
      "source": [
        "Ao observar a matriz de correlação, procuramos por:\n",
        "\n",
        "*   **Correlações Fortes com `income.num` (Variável Alvo):** Variáveis com coeficientes de correlação mais altos (próximos de 1 ou -1) com `income.num` são as mais importantes para prever a renda. Por exemplo, `education.group.num`, `age.group.num`, `capital.result.num` e `marital.status.num` mostram as correlações mais fortes.\n",
        "\n",
        "*   **Multicolinearidade (Correlação entre Variáveis Preditivas):** Se duas variáveis preditivas (não a `income.num`) têm uma correlação muito alta entre si (ex: > 0.7 ou < -0.7), elas podem ser consideradas autocorrelacionadas ou multicolineares. Isso significa que elas fornecem informações redundantes ao modelo e uma delas pode ser removida para simplificar sem perda significativa de performance. Por exemplo, `education.num` e `education.group.num` são altamente correlacionadas, o que é esperado, pois uma é uma versão agrupada da outra.\n",
        "\n",
        "    - No nosso caso, as novas features criadas (`education.group.num`, `workclass.group.num`, `age.group.num`, `capital.result.num`) mostram, como esperado, forte correlação com suas contrapartes originais ou com as variáveis que as compõem. Por exemplo, `capital.result.num` tem correlação alta com `capital.gain.num` e `capital.loss.num`, e `education.group.num` com `education.num`. Isso indica que essas novas features estão encapsulando as informações de suas bases de forma eficiente.\n",
        "\n",
        "    - Para a modelagem, podemos decidir usar apenas as novas features agrupadas, se elas demonstrarem maior poder preditivo ou interpretabilidade, ou as originais, evitando incluir ambas se houver risco de multicolinearidade excessiva que afete a estabilidade do modelo."
      ],
      "id": "e6f9c95c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variáveis sensíveis\n",
        "\n",
        "\n",
        "No contexto deste estudo, é importante destacar que algumas variáveis utilizadas no dataset – em especial raça (race), sexo (sex) e país de origem (native_country) – são consideradas atributos sensíveis, pois se relacionam diretamente a características protegidas por legislações antidiscriminatórias e normas de direitos humanos. A utilização explícita desses campos em modelos de classificação voltados a segmentação de clientes, ainda que com finalidade comercial, aumenta o risco de que o algoritmo aprenda e reproduza desigualdades históricas, gerando decisões com potencial viés discriminatório. Em termos regulatórios, esse tema vem recebendo atenção crescente: na União Europeia, o AI Act e normas complementares de proteção de dados (como o GDPR) estabelecem salvaguardas rigorosas para o uso de dados sensíveis e exigem avaliação de impacto e mitigação de viés em sistemas de IA de “alto risco”; nos Estados Unidos, órgãos como a FTC, CFPB e EEOC têm reiterado que a discriminação algorítmica é tão ilícita quanto a discriminação direta, inclusive em crédito, emprego e serviços financeiros; e no Brasil, a LGPD, aliada a orientações de reguladores do sistema financeiro (como Banco Central, CVM e SUSEP), reforça o princípio de não discriminação e a necessidade de transparência em decisões automatizadas. Diante desse cenário, e alinhado às boas práticas de governança algorítmica, opta-se pela remoção de race, sex e native_country do conjunto de variáveis utilizadas para fins preditivos, de modo a reduzir o risco de que o modelo baseie suas previsões em critérios potencialmente discriminatórios, mantendo o foco em variáveis socioeconômicas e de perfil que sejam mais aceitáveis do ponto de vista ético, regulatório e de reputação institucional.\n",
        "\n",
        "A análise da matriz de correlação indica que a remoção das variáveis sensíveis (race.num, sex.num e native.country.num) tende a ter impacto limitado sobre o poder preditivo global do modelo. Observa-se que race.num e native.country.num apresentam correlações muito baixas com a variável alvo income.num (da ordem de 0,02 a 0,08), sugerindo que, pelo menos de forma linear, contribuem pouco para distinguir clientes de alta renda. Já sex.num apresenta correlação um pouco maior (cerca de 0,22), mas ainda inferior à de variáveis claramente mais informativas, como education.num (≈0,33), education.group.num (≈0,30), capital.gain.num e capital.result.num (≈0,23) e até mesmo hours.per.week (≈0,23). Além disso, parte da informação capturada por sexo e país tende a estar indiretamente refletida em outras variáveis socioeconômicas (escolaridade, tipo de ocupação, horas trabalhadas), o que reduz ainda mais a perda efetiva de performance ao excluí-las. Dessa forma, a expectativa é que a retirada desses atributos sensíveis possa gerar, na prática, apenas uma redução marginal em métricas como AUC ou F1-Score, em troca de um ganho relevante em termos de conformidade regulatória, ética e reputacional, mantendo o modelo baseado principalmente em fatores de perfil econômico e laboral mais alinhados ao objetivo de negócio."
      ],
      "metadata": {
        "id": "OZcXeUyQMSGX"
      },
      "id": "OZcXeUyQMSGX"
    },
    {
      "cell_type": "markdown",
      "id": "20aff970",
      "metadata": {
        "id": "20aff970"
      },
      "source": [
        "## 2.4 Modelagem e Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d1ae870"
      },
      "source": [
        "Nesta seção, utilizaremos uma variedade de algoritmos de Machine Learning para construir e testar modelos preditivos, buscando identificar o que melhor se adapta ao nosso problema de classificação. Os modelos que serão explorados incluem:\n",
        "\n",
        "*   **LogisticRegression (Regressão Logística)**: Um modelo linear simples, mas eficaz, ideal para servir como baseline e oferecer interpretabilidade.\n",
        "*   **KNeighborsClassifier (K-Vizinhos)**: Um algoritmo não-paramétrico baseado em proximidade, que classifica pontos de dados com base na maioria dos votos de seus vizinhos mais próximos.\n",
        "*   **DecisionTreeClassifier (Árvore de Decisão)**: Um modelo intuitivo que usa uma estrutura em forma de árvore para tomar decisões de classificação, facilitando a visualização das regras.\n",
        "*   **RandomForestClassifier (Floresta Aleatória)**: Um método de ensemble que constrói múltiplas árvores de decisão e combina suas previsões para melhorar a acurácia e reduzir o overfitting.\n",
        "*   **GradientBoostingClassifier (Gradient Boosting)**: Outro poderoso método de ensemble que constrói árvores sequencialmente, onde cada nova árvore corrige os erros das árvores anteriores, resultando em modelos de alta performance.\n",
        "*   **SVC (Máquinas de Vetor de Suporte - Support Vector Classifier)**: Um algoritmo que busca o hiperplano ideal para separar as classes, conhecido por sua robustez em espaços de alta dimensão e datasets complexos.\n",
        "\n",
        "Para cada modelo, aplicaremos o GridSearchCV para encontrar e otimizar os melhores hiperparâmetros, garantindo a máxima performance.\n",
        "\n",
        "Os desempenhos dos modelos serão comparados ao final.\n",
        "\n",
        "OBS:. Nenhum dos modelos contém as variáveis sensíveis (race, sex, native_country)"
      ],
      "id": "8d1ae870"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Regressão Logística - Variáveis originais"
      ],
      "metadata": {
        "id": "Lshd-N4uAHlq"
      },
      "id": "Lshd-N4uAHlq"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression  # Importar LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_original = ['age', 'education.num', 'workclass.num',\n",
        "                              'marital.status.num', 'occupation.num',\n",
        "                              'relationship.num', 'capital.gain.num',\n",
        "                              'capital.loss.num', 'hours.per.week',\n",
        "                              'income.num'\n",
        "]\n",
        "\n",
        "df_num_original = df[colunas_numericas_original]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_original.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_original['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base Logistic Regression\n",
        "log_reg_model = LogisticRegression(random_state=42, solver='liblinear', max_iter=200) # Inicializar o modelo Logistic Regression\n",
        "\n",
        "# Definir grade de parâmetros para busca para Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Força de regularização inversa\n",
        "    'penalty': ['l1', 'l2']  # Tipo de regularização\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg_model, # Usar o modelo Logistic Regression\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_lr_original = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_lr_original.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - Regressão Logística\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ul3VA4KRAZGj"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ul3VA4KRAZGj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Regressão Logística - Variáveis novas"
      ],
      "metadata": {
        "id": "YOErQRLNAoKj"
      },
      "id": "YOErQRLNAoKj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression  # Importar LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_new = ['marital.status.num', 'occupation.num',\n",
        "                         'relationship.num', 'capital.result.num',\n",
        "                         'age.group.num', 'education.group.num',\n",
        "                         'workclass.group.num', 'hours.per.week',\n",
        "                         'income.num'\n",
        "]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_new.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_new['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base Logistic Regression\n",
        "log_reg_model_new = LogisticRegression(random_state=42, solver='liblinear', max_iter=200) # Inicializar o modelo Logistic Regression\n",
        "\n",
        "# Definir grade de parâmetros para busca para Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Força de regularização inversa\n",
        "    'penalty': ['l1', 'l2']  # Tipo de regularização\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg_model_new, # Usar o modelo Logistic Regression\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_lr_new = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_lr_new.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - Regressão Logística\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UoWE7W2eAvyE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UoWE7W2eAvyE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: KNN - Variáveis originais"
      ],
      "metadata": {
        "id": "VK6cPU5FOkHL"
      },
      "id": "VK6cPU5FOkHL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier  # Importar KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_original = ['age', 'education.num', 'workclass.num',\n",
        "                              'marital.status.num', 'occupation.num',\n",
        "                              'relationship.num', 'capital.gain.num',\n",
        "                              'capital.loss.num', 'hours.per.week',\n",
        "                              'income.num'\n",
        "]\n",
        "\n",
        "df_num_original = df[colunas_numericas_original]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_original.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_original['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base KNN\n",
        "knn_model = KNeighborsClassifier()  # Inicializar o modelo KNN\n",
        "\n",
        "# Definir grade de parâmetros para busca para KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],  # Número de vizinhos\n",
        "    'weights': ['uniform', 'distance'],  # Peso para os vizinhos\n",
        "    'metric': ['euclidean', 'manhattan']  # Métrica de distância\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=knn_model, # Usar o modelo KNN\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_knn_original = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_knn_original.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - KNN\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2jLF9wfX_Wer"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2jLF9wfX_Wer"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: KNN - Variáveis novas"
      ],
      "metadata": {
        "id": "4efDgnpdOovz"
      },
      "id": "4efDgnpdOovz"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier  # Importar KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_new = ['marital.status.num', 'occupation.num',\n",
        "                         'relationship.num', 'capital.result.num',\n",
        "                         'age.group.num', 'education.group.num',\n",
        "                         'workclass.group.num', 'hours.per.week',\n",
        "                         'income.num'\n",
        "]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_new.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_new['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base KNN\n",
        "knn_model_new = KNeighborsClassifier()  # Inicializar o modelo KNN\n",
        "\n",
        "# Definir grade de parâmetros para busca para KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],  # Número de vizinhos\n",
        "    'weights': ['uniform', 'distance'],  # Peso para os vizinhos\n",
        "    'metric': ['euclidean', 'manhattan']  # Métrica de distância\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=knn_model_new, # Usar o modelo KNN\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_knn_new = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_knn_new.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - KNN\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OcNHm1MHhLAm"
      },
      "id": "OcNHm1MHhLAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Árvore de Decisão - Variáveis originais"
      ],
      "metadata": {
        "id": "iNTqxu09P6kI"
      },
      "id": "iNTqxu09P6kI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49e0307",
      "metadata": {
        "id": "d49e0307"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_original = ['age', 'education.num', 'workclass.num',\n",
        "                              'marital.status.num', 'occupation.num',\n",
        "                              'relationship.num', 'capital.gain.num',\n",
        "                              'capital.loss.num', 'hours.per.week',\n",
        "                              'income.num'\n",
        "]\n",
        "\n",
        "df_num_original = df[colunas_numericas_original]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_original.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_original['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Criar e treinar modelo de Árvore de Decisão\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    criterion='gini',   # ou 'entropy'\n",
        "    max_depth=10,       # limitar profundidade para evitar overfitting\n",
        "    random_state=42\n",
        ")\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - Árvore de Decisão\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Árvore de Decisão - Variáveis novas"
      ],
      "metadata": {
        "id": "D7gCDo74P_75"
      },
      "id": "D7gCDo74P_75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ru0KDCIQJJK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_new = ['marital.status.num', 'occupation.num',\n",
        "                         'relationship.num', 'capital.result.num',\n",
        "                         'age.group.num', 'education.group.num',\n",
        "                         'workclass.group.num', 'hours.per.week',\n",
        "                         'income.num'\n",
        "]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_new.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_new['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Criar e treinar modelo de Árvore de Decisão\n",
        "dt_model_new = DecisionTreeClassifier(\n",
        "    criterion='gini',   # ou 'entropy'\n",
        "    max_depth=10,       # limitar profundidade para evitar overfitting\n",
        "    random_state=42\n",
        ")\n",
        "dt_model_new.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões\n",
        "y_pred = dt_model_new.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - Árvore de Decisão\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "id": "0ru0KDCIQJJK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Floresta Aleatória - Variáveis originais"
      ],
      "metadata": {
        "id": "lbhV-z8dQCnA"
      },
      "id": "lbhV-z8dQCnA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e606e283",
      "metadata": {
        "id": "e606e283"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Seleção das colunas numéricas\n",
        "colunas_numericas_original = ['age', 'education.num', 'workclass.num',\n",
        "                              'marital.status.num', 'occupation.num',\n",
        "                              'relationship.num', 'capital.gain.num',\n",
        "                              'capital.loss.num', 'hours.per.week',\n",
        "                              'income.num'\n",
        "]\n",
        "\n",
        "df_num_original = df[colunas_numericas_original]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_original.drop('income.num', axis=1)\n",
        "y = df_num_original['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir grade de parâmetros para busca\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],       # número de árvores\n",
        "    'max_depth': [10, 20],      # profundidade máxima\n",
        "    'min_samples_split': [2, 5],  # mínimo de amostras para dividir um nó\n",
        "    'min_samples_leaf': [1, 2],    # mínimo de amostras em uma folha\n",
        "    'criterion': ['gini', 'entropy']  # função de impureza\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_rf_original = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_rf_original.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - RandomForest (Original Features)\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Floresta Aleatória - Variáveis novas"
      ],
      "metadata": {
        "id": "IKHmWdoYSCer"
      },
      "id": "IKHmWdoYSCer"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd9e5e7",
      "metadata": {
        "id": "0bd9e5e7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colunas_numericas_new = ['marital.status.num', 'occupation.num',\n",
        "                         'relationship.num', 'capital.result.num',\n",
        "                         'age.group.num', 'education.group.num',\n",
        "                         'workclass.group.num', 'hours.per.week',\n",
        "                         'income.num'\n",
        "]\n",
        "\n",
        "df_num_new = df[colunas_numericas_new]\n",
        "\n",
        "# Definir variáveis independentes (X) e alvo (y)\n",
        "X = df_num_new.drop('income.num', axis=1)   # 'income' é a coluna alvo (>100K ou <=100K)\n",
        "y = df_num_new['income.num']\n",
        "\n",
        "# Dividir em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definir modelo base\n",
        "rf_model_new = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir grade de parâmetros para busca\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],       # número de árvores\n",
        "    'max_depth': [10, 20],      # profundidade máxima\n",
        "    'min_samples_split': [2, 5],  # mínimo de amostras para dividir um nó\n",
        "    'min_samples_leaf': [1, 2],    # mínimo de amostras em uma folha\n",
        "    'criterion': ['gini', 'entropy']  # função de impureza\n",
        "}\n",
        "\n",
        "# Configurar GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,                # validação cruzada com 5 folds\n",
        "    n_jobs=-1,           # usar todos os núcleos\n",
        "    verbose=2            # mostrar progresso\n",
        ")\n",
        "\n",
        "# Treinar com busca em grade\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhor modelo encontrado\n",
        "best_model_rf_new = grid_search.best_estimator_\n",
        "print(\"Melhores parâmetros encontrados:\", grid_search.best_params_)\n",
        "\n",
        "# Fazer previsões com o melhor modelo\n",
        "y_pred = best_model_rf_new.predict(X_test)\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório de Classificação:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Calcular e exibir a matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "plt.title(\"Matriz de Confusão - RandomForest (New Features)\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando olhamos para os resultados destes modelos, fica evidente que não existe “o melhor” em tudo: cada um entrega uma combinação diferente de volume de clientes de alta renda encontrados, qualidade desses leads e custo computacional.\n",
        "\n",
        "**Regressão Logistica: Poucos clientes de alta renda, mas com Boa “Limpeza”**\n",
        "\n",
        "A versão original da Regressão Logística acertou 6.202 clientes de alta renda (VP) e errou 938 (FN), ou seja, deixou passar quase mil clientes que de fato são de alta renda. Em compensação, manteve os falsos positivos relativamente baixos (351), o que significa uma base mais “limpa”: quando ela diz que alguém é alta renda, a chance de estar certa é razoável. A versão com variáveis novas não melhorou o quadro de forma significativa: 6.244 VP (42 a mais) e 765 FN (uma redução de 173 clientes de alta renda perdidos), mas à custa de mais mudanças no balanço geral. Mesmo com ROC-AUC razoável (0,85 no original), a Regressão Logística mostra uma limitação clara: captura pouco da não-linearidade do perfil de alta renda, deixando muitas oportunidades na mesa.\n",
        "\n",
        "**KNN: Mais Agressivo, mas Ainda Aquém**\n",
        "\n",
        "O KNN original encontrou 5.862 clientes de alta renda (VP), perdendo 1.188 (FN), e cometendo 691 falsos positivos. Ou seja: ele tenta ser mais agressivo que a Regressão Logística, mas ainda erra bastante dos dois lados. A versão com variáveis novas melhora ligeiramente o equilíbrio: 5.949 VP (87 a mais), 1.139 FN (49 clientes de alta renda a mais corretamente identificados) e 604 FP (menos abordagens erradas). O problema é o custo: ~45,6 segundos na versão original e 22,37s na nova, para uma performance global (F1 em torno de 0,63 → 0,62 e ROC-AUC caindo de ~0,88 para ~0,86) que continua atrás dos modelos de árvore. Em resumo: KNN trabalha muito, gasta tempo e não entrega um ganho competitivo.\n",
        "\n",
        "**Decision Tree: Simples, Razoavelmente Forte e Muito Barata**\n",
        "\n",
        "A Decision Tree com variáveis originais encontra 5.962 clientes de alta renda e deixa escapar 1.410 (FN). Ao trocar para as novas variáveis, o modelo passa a encontrar 6.094 clientes de alta renda (132 a mais) e reduz os FN para 1.255 (155 clientes de alta renda a mais corretamente identificados), com uma precision ainda sólida. O tempo de treino continua irrisório (0,04s → 0,05s), e o F1-Score se mantém em torno de 0,68 (original) e 0,63 (novo), com ROC-AUC perto de 0,89. Isso significa que a árvore, apesar de simples, captura bem a lógica de segmentação e é extremamente econômica em processamento. Ela erra mais do que a Random Forest, mas faz isso quase de graça em termos de custo computacional.\n",
        "\n",
        "**Random Forest: Modelo Forte, mas com Custo de Máquina Elevado**\n",
        "\n",
        "A Random Forest com variáveis originais é, tecnicamente, o modelo mais robusto desse conjunto: 6.228 clientes de alta renda corretamente classificados (maior VP absoluto), 1.198 perdidos (FN) e apenas 325 falsos positivos. Ou seja, encontra muito cliente de alta renda e erra relativamente pouco dos dois lados. O ROC-AUC de ~0,92 confirma a alta capacidade de discriminação e o F1-Score mostra bom equilíbrio entre precisão e recall. O preço disso? Tempo: cerca de 46 segundos por treino. Na versão com variáveis novas, a Random Forest perde um pouco de performance e passa a 6.121 VP e 1.315 FN, seja, perde 107 clientes de alta renda e aumenta a quantidade de clientes de alta renda ignorados. Além disso, mantém custo computacional elevado. Em termos práticos: a Random Forest original é o “modelo forte”, mas caro; a versão com novas variáveis não justifica a troca.\n",
        "\n",
        "**Resumo: Dentre estes modelos, qual faria mais sentido para o Banco?**\n",
        "\n",
        "Se o objetivo do banco é maximizar a captação de clientes de alta renda com base em um modelo tecnicamente confiável, a Random Forest com variáveis originais se destaca como o melhor compromisso entre volume de clientes de alta renda encontrados (maior VP), qualidade dos leads (boa precisão, poucos FP) e robustez estatística (maior ROC-AUC). A Decision Tree aparece como uma alternativa extremamente barata computacionalmente, com desempenho próximo em termos de F1-Score e recall, servindo bem como modelo explicável e leve. Já KNN e Regressão Logística, embora úteis como benchmarks, mostram limitações claras: ou deixam muitos clientes de alta renda de fora (FN altos), ou exigem muito processamento para entregar menos do que as árvores conseguem. Em termos de inteligência de negócios, isso significa que a floresta é a estratégia mais efetiva para transformar o perfil de investidor em ação comercial concreta, enquanto os demais modelos ficam melhor posicionados como apoio comparativo e não como solução de produção."
      ],
      "metadata": {
        "id": "8HMsfvKocrAx"
      },
      "id": "8HMsfvKocrAx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "### Gradiente Boosting e SVC\n",
        "\n"
      ],
      "metadata": {
        "id": "U577iaSSVQBr"
      },
      "id": "U577iaSSVQBr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação do Ambiente e Processamento Final\n",
        "\n",
        "Nesta etapa, consolidamos o pré-processamento dos dados. Definimos as variáveis preditoras (**X**) e a variável alvo (**y**).\n",
        "Em seguida, realizamos a divisão entre dados de Treino (para o modelo aprender) e Teste (para validar a performance), além de aplicar a **Padronização (StandardScaler)**.\n",
        "\n",
        "A padronização é crucial, especialmente para o SVC, pois coloca todas as variáveis (idade, horas trabalhadas, capital) na mesma escala matemática."
      ],
      "metadata": {
        "id": "fwiQWxMx9R1V"
      },
      "id": "fwiQWxMx9R1V"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- FUNÇÃO AUXILIAR ATUALIZADA (COM TEMPO) ---\n",
        "def avaliar_modelo(modelo, X_test, y_test, nome_modelo, tempo_treino):\n",
        "    \"\"\"Calcula métricas, exibe Matriz de Confusão e retorna dicionário com Tempo.\"\"\"\n",
        "\n",
        "    # Predições\n",
        "    y_pred = modelo.predict(X_test)\n",
        "    try:\n",
        "        y_proba = modelo.predict_proba(X_test)[:, 1]\n",
        "        roc = roc_auc_score(y_test, y_proba)\n",
        "    except:\n",
        "        roc = \"N/A\"\n",
        "\n",
        "    # Print do Log\n",
        "    print(f\"=== RESULTADOS: {nome_modelo} ===\")\n",
        "    print(f\"⏱️ Tempo de Treino: {tempo_treino:.2f} segundos\")\n",
        "    print(f\"Acurácia:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"ROC-AUC:   {roc if isinstance(roc, str) else f'{roc:.4f}'}\")\n",
        "\n",
        "    # Matriz de Confusão\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['<=100K', '>100K'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
        "    plt.title(f\"Matriz: {nome_modelo}\")\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "    # Retorna dicionário completo para a tabela final\n",
        "    return {\n",
        "        \"Modelo\": nome_modelo,\n",
        "        \"Acurácia\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1-Score\": f1_score(y_test, y_pred),\n",
        "        \"ROC-AUC\": roc if not isinstance(roc, str) else 0,\n",
        "        \"Tempo (s)\": round(tempo_treino, 2)  # <--- NOVA COLUNA\n",
        "    }\n",
        "\n",
        "# --- PREPARAÇÃO DOS DADOS (X e y) ---\n",
        "colunas_features = [\n",
        "    'age', 'workclass.num', 'education.num', 'marital.status.num',\n",
        "    'occupation.num', 'relationship.num', 'hours.per.week', 'capital.result'\n",
        "]\n",
        "X = df[colunas_features]\n",
        "y = df['income.num']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_final = pd.DataFrame(X_train_scaled, columns=colunas_features)\n",
        "X_test_final = pd.DataFrame(X_test_scaled, columns=colunas_features)\n",
        "\n",
        "# Lista para acumular resultados\n",
        "historico_modelos = []\n",
        "\n",
        "print(\"✅ Ambiente preparado e função de log configurada.\")"
      ],
      "metadata": {
        "id": "01n3pBvd9Y7j"
      },
      "id": "01n3pBvd9Y7j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ajustando modelos anteriores para permitir comparação"
      ],
      "metadata": {
        "id": "gvyGdCE_4kLQ"
      },
      "id": "gvyGdCE_4kLQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Logistic Regression (Original)\n",
        "inicio = time.time()\n",
        "best_model_lr_original.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_lr_orig = fim - inicio\n",
        "\n",
        "res_lr_orig = avaliar_modelo(\n",
        "    best_model_lr_original,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Logistic Regression (Original)\",\n",
        "    tempo_lr_orig\n",
        ")\n",
        "historico_modelos.append(res_lr_orig)\n",
        "\n",
        "# Logistic Regression (New)\n",
        "inicio = time.time()\n",
        "best_model_lr_new.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_lr_new = fim - inicio\n",
        "\n",
        "res_lr_new = avaliar_modelo(\n",
        "    best_model_lr_new,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Logistic Regression (New)\",\n",
        "    tempo_lr_new\n",
        ")\n",
        "historico_modelos.append(res_lr_new)\n",
        "\n",
        "# KNN (Original)\n",
        "inicio = time.time()\n",
        "best_model_knn_original.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_knn_orig = fim - inicio\n",
        "\n",
        "res_knn_orig = avaliar_modelo(\n",
        "    best_model_knn_original,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"KNN (Original)\",\n",
        "    tempo_knn_orig\n",
        ")\n",
        "historico_modelos.append(res_knn_orig)\n",
        "\n",
        "# KNN (New)\n",
        "inicio = time.time()\n",
        "best_model_knn_new.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_knn_new = fim - inicio\n",
        "\n",
        "res_knn_new = avaliar_modelo(\n",
        "    best_model_knn_new,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"KNN (New)\",\n",
        "    tempo_knn_new\n",
        ")\n",
        "historico_modelos.append(res_knn_new)\n",
        "\n",
        "# Decision Tree (Original)\n",
        "inicio = time.time()\n",
        "dt_model_original.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_dt_orig = fim - inicio\n",
        "\n",
        "res_dt_orig = avaliar_modelo(\n",
        "    dt_model_original,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Decision Tree (Original)\",\n",
        "    tempo_dt_orig\n",
        ")\n",
        "historico_modelos.append(res_dt_orig)\n",
        "\n",
        "# Decision Tree (New)\n",
        "inicio = time.time()\n",
        "dt_model_new.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_dt_new = fim - inicio\n",
        "\n",
        "res_dt_new = avaliar_modelo(\n",
        "    dt_model_new,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Decision Tree (New)\",\n",
        "    tempo_dt_new\n",
        ")\n",
        "historico_modelos.append(res_dt_new)\n",
        "\n",
        "# Random Forest (Original)\n",
        "inicio = time.time()\n",
        "best_model_rf_original.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_rf_orig = fim - inicio\n",
        "\n",
        "res_rf_orig = avaliar_modelo(\n",
        "    best_model_rf_original,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Random Forest (Original)\",\n",
        "    tempo_rf_orig\n",
        ")\n",
        "historico_modelos.append(res_rf_orig)\n",
        "\n",
        "# Random Forest (New)\n",
        "inicio = time.time()\n",
        "best_model_rf_new.fit(X_train_final, y_train)\n",
        "fim = time.time()\n",
        "tempo_rf_new = fim - inicio\n",
        "\n",
        "res_rf_new = avaliar_modelo(\n",
        "    best_model_rf_new,\n",
        "    X_test_final,\n",
        "    y_test,\n",
        "    \"Random Forest (New)\",\n",
        "    tempo_rf_new\n",
        ")\n",
        "historico_modelos.append(res_rf_new)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qqc2Pri74Q4V"
      },
      "id": "qqc2Pri74Q4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: Gradient Boosting (Baseline / Default)\n",
        "\n",
        "Iniciamos a modelagem utilizando o **Gradient Boosting Classifier** com seus hiperparâmetros padrão (default).\n",
        "Este modelo constrói árvores de decisão sequencialmente, onde cada nova árvore tenta corrigir os erros da anterior. O objetivo aqui é estabelecer um \"piso\" de desempenho para compararmos posteriormente com a versão otimizada.\n",
        "\n",
        "Analisaremos a **Matriz de Confusão** para entender quantos clientes de Alta Renda (>50K) o modelo acertou (Verdadeiros Positivos) e quantos ele deixou escapar (Falsos Negativos)."
      ],
      "metadata": {
        "id": "u_0jMDaO9jEb"
      },
      "id": "u_0jMDaO9jEb"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"⏳ Iniciando treino GB Default...\")\n",
        "\n",
        "# 1. Inicia Cronômetro\n",
        "start = time.time()\n",
        "\n",
        "gb_default = GradientBoostingClassifier(random_state=42)\n",
        "gb_default.fit(X_train_final, y_train)\n",
        "\n",
        "# 2. Para Cronômetro\n",
        "end = time.time()\n",
        "tempo_gb_def = end - start\n",
        "\n",
        "# 3. Avalia passando o tempo\n",
        "res_gb_def = avaliar_modelo(gb_default, X_test_final, y_test, \"Gradient Boosting (Default)\", tempo_gb_def)\n",
        "historico_modelos.append(res_gb_def)"
      ],
      "metadata": {
        "id": "Ei1iLtxj9mzT"
      },
      "id": "Ei1iLtxj9mzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Otimização de Esforço (A Grande Caixa Azul Escura - 5.136)**\n",
        "O modelo identificou corretamente **5.136 pessoas** que *não* possuem perfil para produtos de alta renda.\n",
        "*   **Ganho:** A equipe comercial deixou de fazer 5.136 ligações inúteis. Isso libera nossos gerentes para focar apenas em quem realmente traz retorno.\n",
        "\n",
        "**2. O \"Tiro de Precisão\" (A Caixa Azul Clara Inferior - 1.068)**\n",
        "O modelo nos entregou uma lista de **1.084 clientes** com alto potencial de investimento confirmados.\n",
        "*   **Ganho:** São mais de mil oportunidades reais de venda (leads qualificados) que poderiam estar perdidas na base de dados.\n",
        "\n",
        "**3. Baixo Desperdício de Marketing (O Número 323)**\n",
        "O modelo errou ao sugerir que **309 pessoas** eram ricas, quando não eram.\n",
        "*   **Impacto:** Em um universo de 7.000 clientes, errar apenas 323 vezes é um custo marginal aceitável. Significa que nossa **Taxa de Assertividade na Oferta** é altíssima (quase 80%). Cada vez que o modelo diz \"ligue para este cliente\", ele tem uma chance muito alta de estar certo.\n",
        "\n",
        "**4. Onde Podemos Melhorar (O Número 714)**\n",
        "O modelo foi conservador e deixou de identificar **714 clientes ricos** (classificando-os como renda baixa).\n",
        "*   **Oportunidade:** Isso é dinheiro deixado na mesa. Porém, como este é apenas o modelo \"padrão\" (sem ajustes), temos uma margem clara para capturar esses clientes na próxima etapa de otimização.\n",
        "\n",
        "**Conclusão Gerencial:**\n",
        "A ferramenta é extremamente rápida (processamento instantâneo) e já no seu formato básico consegue filtrar nossa base com **85% de acerto global**, protegendo a equipe de vendas de desperdiçar tempo e garantindo foco nos clientes certos."
      ],
      "metadata": {
        "id": "-HskyVLsCYuq"
      },
      "id": "-HskyVLsCYuq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimização do Gradient Boosting (Tuning)\n",
        "\n",
        "Embora o modelo padrão seja bom, podemos melhorar seu desempenho ajustando os hiperparâmetros. Utilizaremos o **RandomizedSearchCV**, que testa combinações aleatórias de parâmetros para encontrar o melhor ajuste sem consumir tempo excessivo.\n",
        "\n",
        "**Parâmetros focados:**\n",
        "*   `n_estimators`: Número de árvores de decisão.\n",
        "*   `learning_rate`: O quanto cada árvore contribui para a correção de erros.\n",
        "*   `max_depth`: A complexidade máxima de cada árvore.\n",
        "\n",
        "O objetivo principal é tentar aumentar o **Recall** e o **F1-Score**, garantindo que identifiquemos mais clientes de alta renda sem perder precisão."
      ],
      "metadata": {
        "id": "F8Y3vQnG9r5T"
      },
      "id": "F8Y3vQnG9r5T"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"⏳ Iniciando Otimização GB (Random Search)...\")\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# 1. Inicia Cronômetro\n",
        "start = time.time()\n",
        "\n",
        "gb_search = RandomizedSearchCV(\n",
        "    estimator=GradientBoostingClassifier(random_state=42),\n",
        "    param_distributions=param_grid_gb,\n",
        "    n_iter=10, scoring='roc_auc', cv=3, random_state=42, n_jobs=-1\n",
        ")\n",
        "gb_search.fit(X_train_final, y_train)\n",
        "\n",
        "# 2. Para Cronômetro\n",
        "end = time.time()\n",
        "tempo_gb_opt = end - start\n",
        "\n",
        "gb_best = gb_search.best_estimator_\n",
        "print(f\"✅ Melhores parâmetros: {gb_search.best_params_}\")\n",
        "\n",
        "# 3. Avalia passando o tempo\n",
        "res_gb_opt = avaliar_modelo(gb_best, X_test_final, y_test, \"Gradient Boosting (Otimizado)\", tempo_gb_opt)\n",
        "historico_modelos.append(res_gb_opt)"
      ],
      "metadata": {
        "id": "Qkf3bYvm9vgb",
        "collapsed": true
      },
      "id": "Qkf3bYvm9vgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Resultado da Otimização:**\n",
        "\n",
        "**1. O Ganho de Receita (A Caixa Azul Clara Inferior - 1.187)**\n",
        "Aqui está o grande salto. O modelo anterior encontrou 1.084 clientes de alta renda. Este novo modelo encontrou **1.187**.\n",
        "*   **Impacto Financeiro:** \"Resgatamos\" **103 clientes de alto valor** que o modelo anterior ignorava. Se considerarmos que cada cliente desse perfil aporta, em média, R$ 100 mil, estamos falando de uma **Receita Incremental de R$$ 10,3 Milhões** identificada apenas por ajustar o algoritmo, sem gastar um centavo a mais em marketing.\n",
        "\n",
        "**2. Redução do \"Dinheiro na Mesa\" (A Redução de 714 para 611)**\n",
        "Conseguimos reduzir drasticamente o número de clientes ricos que o banco classificava erroneamente como renda baixa.\n",
        "*   **Ganho:** Diminuímos nosso \"ponto cego\". Estamos aproveitando muito melhor a nossa própria base de dados.\n",
        "\n",
        "**3. O Custo Operacional Controlado (381 Erros)**\n",
        "Para capturar esses 103 clientes extras, o modelo ficou um pouco mais \"ousado\" e errou um pouco mais (subiu de 323 para 381 falsos positivos).\n",
        "*   **Análise de ROI:** Vale a pena fazer 58 ligações a mais (custo operacional baixo) para fechar 103 novas contas Premium? A resposta é um **SIM** absoluto. O retorno sobre o investimento (ROI) dessa troca é extremamente positivo.\n",
        "\n",
        "**4. Confiabilidade Nível \"A\" (ROC-AUC 0.92)**\n",
        "Atingimos uma pontuação de **0.92** (em uma escala de 0 a 1).\n",
        "*   **Tradução:** Este é um nível de excelência raramente visto em modelos de comportamento humano. Podemos confiar que a segmentação feita por este sistema é robusta, estável e segura para ser implantada em larga escala.\n",
        "\n",
        "**Veredito:**\n",
        "Este é o modelo maximiza a captação de clientes ricos (Receita) mantendo o desperdício operacional (Custo) sob total controle. Estamos prontos para a implantação."
      ],
      "metadata": {
        "id": "Wt9IgMoqEOnq"
      },
      "id": "Wt9IgMoqEOnq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo: SVC - Support Vector Classifier (Default)\n",
        "\n",
        "Agora introduzimos um modelo de família diferente: o **SVC**. Este algoritmo busca encontrar o melhor hiperplano (fronteira) que separa as classes no espaço multidimensional.\n",
        "\n",
        "O SVC é conhecido por sua alta precisão em margens complexas, mas possui um alto custo computacional. Testaremos a versão padrão com kernel `rbf` (Radial Basis Function) para verificar como ele se compara ao Gradient Boosting em termos de precisão e tempo."
      ],
      "metadata": {
        "id": "eyAUVZKn-Yaz"
      },
      "id": "eyAUVZKn-Yaz"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"⏳ Iniciando treino SVC Default \")\n",
        "\n",
        "# 1. Inicia Cronômetro\n",
        "start = time.time()\n",
        "\n",
        "svc_default = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svc_default.fit(X_train_final, y_train)\n",
        "\n",
        "# 2. Para Cronômetro\n",
        "end = time.time()\n",
        "tempo_svc_def = end - start\n",
        "\n",
        "# 3. Avalia\n",
        "res_svc_def = avaliar_modelo(svc_default, X_test_final, y_test, \"SVC (Default)\", tempo_svc_def)\n",
        "historico_modelos.append(res_svc_def)"
      ],
      "metadata": {
        "id": "vxmGkEHO-biz"
      },
      "id": "vxmGkEHO-biz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  \n",
        "\n",
        "**1. O Gargalo de Produção (O Tempo: 73.83 Segundos)**\n",
        "\n",
        "Os números indicam que, embora seja uma ferramenta robusta, ela apresenta **duas desvantagens críticas** para a nossa operação quando comparada ao modelo anterior (Gradient Boosting).\n",
        "\n",
        "Este é o ponto de atenção imediato. Enquanto o modelo anterior levava menos de 2 segundos para processar a base, este levou **73 segundos**.\n",
        "\n",
        "*   **Impacto de TI:** Estamos falando de um processo **35 vezes mais lento**.\n",
        "\n",
        "Em uma base de milhões de clientes, isso significa que nossos servidores ficariam ocupados por horas, atrasando a entrega de leads para a equipe comercial. É uma solução difícil de escalar.\n",
        "\n",
        "**2. Queda na Captação de Receita (A Caixa Azul Clara Inferior - 913)**\n",
        "O SVC identificou **913 clientes** de alta renda.\n",
        "\n",
        "*   **A \"Perda\" Invisível:** Comparado ao nosso modelo campeão (que achou 1.187), este modelo **deixou de identificar 274 clientes de alto valor**.\n",
        "*   **Custo de Oportunidade:** Estamos deixando de ofertar produtos para 274 pessoas ricas simplesmente porque o modelo não conseguiu \"enxergá-las\". Se cada cliente vale 'R$ 100 mil', este modelo está, na prática, nos fazendo **perder R$ 27,4 milhões em potencial** comparado à opção anterior.\n",
        "\n",
        "**3. Excesso de Cautela (O Número 885)**\n",
        "\n",
        "O modelo errou ao classificar **885 clientes ricos** como renda baixa.\n",
        "\n",
        "*   **Interpretação:** Este algoritmo \"joga na defesa\". Ele prefere não arriscar um palpite a menos que tenha certeza absoluta. Embora isso pareça seguro, para vendas é ruim: preferimos um modelo que nos aponte oportunidades, mesmo que erre um pouco mais, do que um modelo que se cala diante de um bom negócio.\n",
        "\n",
        "**4. O Lado Positivo: Baixo Custo Operacional (333 Erros)**\n",
        "\n",
        "O único ponto onde ele brilha é na economia de ligações erradas (apenas 333).\n",
        "*   **Trade-off:** Ele economiza o tempo do gerente em 59 ligações (comparado ao otimizado), mas ao custo de perder 274 contratos. A conta não fecha. É uma economia \"burra\".\n",
        "\n",
        "**Conclusão**\n",
        "O SVC é uma tecnologia sólida, mas para este desafio específico, ele se mostrou **lento demais e conservador demais**. Ele protege a operação de erros, mas sufoca a receita. O Gradient Boosting continua sendo nossa escolha superior."
      ],
      "metadata": {
        "id": "y0pHVUeMHmrr"
      },
      "id": "y0pHVUeMHmrr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimização do SVC com GridSearchCV\n",
        "\n",
        "Para tentar extrair o máximo do SVC, utilizaremos o **GridSearchCV**. Diferente da busca aleatória, o GridSearch testa **todas** as combinações possíveis que definirmos.\n",
        "\n",
        "⚠️ **Atenção Computacional:** Como o SVC é naturalmente lento em datasets grandes (>10k linhas), definiremos uma grade de parâmetros restrita (poucas opções de `C` e `gamma`) para evitar que o tempo de execução se torne inviável nesta simulação.\n",
        "\n",
        "O objetivo é verificar se, com o ajuste fino, o SVC consegue superar o F1-Score do Gradient Boosting."
      ],
      "metadata": {
        "id": "C2a_4NSv_Du0"
      },
      "id": "C2a_4NSv_Du0"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"⏳ Iniciando GridSearchCV no SVC (Atenção: Processo demorado)...\")\n",
        "\n",
        "param_grid_svc = {\n",
        "    'C': [1, 10],            # Reduzi opções para não demorar demais\n",
        "    'gamma': ['scale'],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# 1. Inicia Cronômetro\n",
        "start = time.time()\n",
        "\n",
        "grid_svc = GridSearchCV(\n",
        "    estimator=SVC(probability=True, random_state=42),\n",
        "    param_grid=param_grid_svc,\n",
        "    scoring='f1', cv=2, n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_svc.fit(X_train_final, y_train)\n",
        "\n",
        "# 2. Para Cronômetro\n",
        "end = time.time()\n",
        "tempo_svc_opt = end - start\n",
        "\n",
        "svc_best = grid_svc.best_estimator_\n",
        "print(f\"✅ GridSearch SVC finalizado.\")\n",
        "\n",
        "# 3. Avalia\n",
        "res_svc_opt = avaliar_modelo(svc_best, X_test_final, y_test, \"SVC (GridSearch)\", tempo_svc_opt)\n",
        "historico_modelos.append(res_svc_opt)"
      ],
      "metadata": {
        "id": "SXAudlV8_JFz"
      },
      "id": "SXAudlV8_JFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Inviabilidade Operacional (O Tempo: 179,47 Segundos)**\n",
        "\n",
        "Este é o número que inviabiliza o projeto com esta tecnologia.\n",
        "\n",
        "*   **O Problema:** Levamos **3 minutos** para processar esta pequena amostra. Se aplicarmos isso à nossa base real de milhões de clientes, o processamento demoraria dias.\n",
        "*   **Comparativo:** O modelo campeão (Gradient Boosting) faz o mesmo trabalho com muito mais qualidade em uma fração desse tempo.  \n",
        "\n",
        "**2. Um Esforço Grande para um Ganho Pequeno (953 Clientes)**\n",
        "Com todo esse esforço de processamento, o modelo conseguiu encontrar **953 clientes** de alta renda.\n",
        "\n",
        "*   **A Melhora:** Sim, foi melhor que a versão básica do SVC (que achou 921). Ganhamos 32 clientes.\n",
        "*   **A Derrota:** Mesmo com essa melhora, ele ainda está **anos-luz atrás** do nosso modelo campeão (Gradient Boosting Otimizado), que encontrou **1.191 clientes**.\n",
        "*   **Resumo:** Trabalhamos o triplo do tempo para ainda entregar um resultado **20% inferior** em captação de receita.\n",
        "\n",
        "**3. O Custo de Oportunidade Continua Alto (845 Perdidos)**\n",
        "O modelo continua ignorando **845 clientes ricos** (classificando-os como renda baixa).\n",
        "\n",
        "*   **Impacto:** Continuamos deixando dinheiro na mesa. A tecnologia SVC, por sua natureza matemática, tem dificuldade em ser \"agressiva\" na busca por clientes sem comprometer a precisão.\n",
        "\n",
        "**CONCLUSÃO**\n",
        "\n",
        "1.  **Descartamos o SVC:** Seja na versão básica ou otimizada, ele é lento, custoso computacionalmente e entrega menos receita (menos leads qualificados).\n",
        "2.  **Elegemos o Gradient Boosting Otimizado:** Este será o modelo de produção.\n",
        "    *   Ele nos traz **1.191 clientes de alta renda** (o maior volume de todos os testes).\n",
        "    *   Ele possui a maior confiabilidade técnica (**ROC-AUC 0.92**).\n",
        "    *   Ele é rápido e escalável.\n"
      ],
      "metadata": {
        "id": "ZJOTj_RcJ8-J"
      },
      "id": "ZJOTj_RcJ8-J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Análise Comparativa e Teorização\n",
        "\n",
        "Abaixo, consolidamos os resultados dos 4 cenários testados.\n",
        "Nossa decisão final deve equilibrar **Desempenho Técnico (F1-Score e AUC)** com **Viabilidade de Negócio (Tempo de Treino/Produção)**.\n",
        "\n",
        "Observamos os trade-offs entre a capacidade do modelo de encontrar clientes de alta renda (Recall) e a precisão dessa classificação."
      ],
      "metadata": {
        "id": "ST1kYTp-_Nyb"
      },
      "id": "ST1kYTp-_Nyb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DataFrame Final\n",
        "df_final = pd.DataFrame(historico_modelos)\n",
        "df_final = df_final.set_index(\"Modelo\").sort_values(by=\"F1-Score\", ascending=False)\n",
        "\n",
        "print(\"=== TABELA FINAL DE RESULTADOS (COM TEMPO) ===\")\n",
        "display(df_final)\n",
        "\n",
        "# Transforma o índice em coluna para o seaborn\n",
        "df_final = df_final.reset_index()  # cria coluna 'Modelo'\n",
        "\n",
        "# Gráfico Comparativo de Tempo vs Performance\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Barras de F1-Score\n",
        "sns.barplot(x='Modelo', y='F1-Score', data=df_final, ax=ax1,\n",
        "            palette='viridis', alpha=0.6)\n",
        "ax1.set_ylabel('F1-Score (Barras)', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Linha de tempo\n",
        "ax2 = ax1.twinx()\n",
        "sns.lineplot(x='Modelo', y='Tempo (s)', data=df_final, ax=ax2,\n",
        "             color='red', marker='o', sort=False)\n",
        "ax2.set_ylabel('Tempo em Segundos (Linha Vermelha)', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "plt.title(\"Trade-off: Performance (F1) vs Custo Computacional (Tempo)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QOLXKTDA_NDT"
      },
      "id": "QOLXKTDA_NDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos comparar o desempenho do Gradient Boosting (Otimizado), melhor modelo na maioria das métricas, e do Random Forest (Original), que teve melhor precisão, e entender qual seria a melhor escolha para o negócio de segmentação de clientes para oferta de fundos de investimento.\n",
        "\n",
        "Com base nos resultados consolidados na tabela final, temos as seguintes métricas para os dois modelos:\n",
        "\n",
        "1. Gradient Boosting (Otimizado):\n",
        "\n",
        "Acurácia: 0.8633\n",
        "Precisão: 0.7570\n",
        "Recall: 0.6602\n",
        "F1-Score: 0.7053\n",
        "ROC-AUC: 0.9212\n",
        "Tempo de Treino (GridSearch): 98.34 segundos\n",
        "\n",
        "2. Random Forest (Original):\n",
        "\n",
        "Acurácia: 0.8521\n",
        "Precisão: 0.7972\n",
        "Recall: 0.5403\n",
        "F1-Score: 0.6441\n",
        "ROC-AUC: 0.9161\n",
        "Tempo de Treino (GridSearch): 46.27 segundos\n",
        "Análise Comparativa e Percepção de Negócio:\n",
        "\n",
        "\n",
        "Foco na Captação de Alta Renda (Recall e F1-Score):\n",
        "\n",
        "O Gradient Boosting (Otimizado) apresenta um Recall significativamente maior (0.6602) em comparação ao Random Forest (Original) (0.5403). Isso significa que o Gradient Boosting é muito mais eficaz em identificar uma porcentagem maior dos clientes que realmente possuem alta renda.\n",
        "Para um negócio de fundos de investimento, não perder um cliente de alta renda em potencial é crucial. Cada cliente de alta renda representa um volume considerável de aporte e receita para a instituição. Um recall mais alto se traduz diretamente em mais leads qualificados para a equipe comercial.\n",
        "O F1-Score do Gradient Boosting (0.7053) também é superior ao do Random Forest (0.6441), indicando um melhor equilíbrio geral entre Precisão e Recall, mas com uma forte inclinação para capturar mais positivos reais, o que é benéfico neste cenário.\n",
        "Qualidade dos Leads (Precisão):\n",
        "\n",
        "O Random Forest (Original) tem uma Precisão ligeiramente superior (0.7972) em relação ao Gradient Boosting (0.7570). Isso indica que, quando o Random Forest classifica um cliente como alta renda, ele tem uma probabilidade um pouco maior de estar correto.\n",
        "No entanto, a diferença de precisão é relativamente pequena e, no contexto de busca por clientes de alto valor, um recall mais alto (identificando mais oportunidades) muitas vezes compensa uma precisão marginalmente menor (ter alguns falsos positivos a mais).\n",
        "Capacidade de Discriminação (ROC-AUC):\n",
        "\n",
        "Ambos os modelos demonstram excelente capacidade de distinguir entre as classes, com o Gradient Boosting (0.9212) tendo uma pequena vantagem sobre o Random Forest (0.9161). Isso reforça a robustez de ambos, mas o GB ainda se destaca marginalmente.\n",
        "Custo Computacional (Tempo de Treino):\n",
        "\n",
        "O Random Forest (Original) teve um tempo de otimização de hiperparâmetros (GridSearchCV) mais rápido (46.27 segundos) do que o Gradient Boosting (Otimizado) (98.34 segundos). É importante notar que esses são tempos de tuning, e não o tempo de inferência (aplicação do modelo treinado a novos dados), que para ambos os modelos é muito baixo. Para a fase de produção, o tempo de inferência é o mais relevante, e ambos seriam rápidos.\n",
        "Conclusão para o Negócio de Investimentos:\n",
        "\n",
        "Considerando o objetivo de segmentar clientes para a oferta de fundos de investimento de alta renda, o Gradient Boosting (Otimizado) é o modelo mais vantajoso.\n",
        "\n",
        "Embora o Random Forest (Original) tenha uma precisão marginalmente melhor e um tempo de tuning mais rápido, a capacidade superior do Gradient Boosting em identificar um volume maior de clientes de alta renda (recall mais alto), traduzida por seu F1-Score e ROC-AUC superiores, é um diferencial crítico. Para o banco, maximizar a detecção de potenciais investidores de alto valor é o principal motor de receita, mesmo que isso signifique uma taxa ligeiramente maior de \"falsos positivos\" que a equipe de vendas possa filtrar. O custo-benefício de encontrar um número significativamente maior de clientes que realmente se encaixam no perfil de alta renda supera o pequeno ganho de precisão do Random Forest. A alta performance geral e a escalabilidade (rápida inferência após o tuning) do Gradient Boosting o tornam a escolha mais estratégica para esta finalidade comercial."
      ],
      "metadata": {
        "id": "I8mkuR2F7ncn"
      },
      "id": "I8mkuR2F7ncn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.6 Conclusão e Próximos Passos\n",
        "\n",
        "### A Supremacia do Gradient Boosting\n",
        "\n",
        "\n",
        "Temos duas métricas conflitantes em jogo:\n",
        "1.  **As Barras (F1-Score):** Representam a **Qualidade da Receita**. Quanto mais alta a barra, mais dinheiro o banco ganha acertando os clientes ricos.\n",
        "2.  **A Linha Vermelha (Tempo):** Representa o **Custo Operacional**. Quanto mais alta a linha, mais caro e lento é manter o sistema.\n",
        "\n",
        "**A Análise do Vencedor (Lado Esquerdo do Gráfico)**\n",
        "Observem as duas primeiras barras (Gradient Boosting).\n",
        "*   **Qualidade Máxima:** A barra do \"Gradient Boosting (Otimizado)\" é a mais alta de todas (**F1 de 0.70**). Isso significa que é o modelo que melhor equilibra precisão e volume de vendas.\n",
        "*   **Eficiência Oculta:** Embora o ponto vermelho do modelo otimizado esteja alto (105s), isso representa apenas o tempo de *pesquisa* (setup). Uma vez configurado, ele rodará na velocidade do \"Gradient Boosting Default\" (ponto vermelho no chão: **1,84 segundos**).\n",
        "*   **Resumo:** Temos a qualidade máxima com um custo de execução irrisório.\n",
        "\n",
        "**A Análise do Perdedor (Lado Direito do Gráfico)**\n",
        "Observem as duas últimas barras (SVC).\n",
        "*   **A \"Boca de Jacaré\" Invertida:** Temos o pior cenário possível. As barras de qualidade são as mais baixas (F1 de ~0.60), enquanto a linha vermelha de custo explode para o topo (**180 segundos**).\n",
        "*   **Ineficiência:** Estamos pagando 100 vezes mais tempo computacional para ter um resultado 15% pior. Em qualquer negócio, isso justifica o cancelamento imediato da tecnologia.\n",
        "\n",
        "**O Diferencial de Mercado (Recall)**\n",
        "Olhem para a coluna \"Recall\" na tabela.\n",
        "*   **GB Otimizado:** 0.66 (Captura 66% de todos os ricos).\n",
        "*   **SVC Grid:** 0.53 (Captura 53% de todos os ricos).\n",
        "*   **Impacto:** O modelo vencedor coloca **13% mais clientes de alta renda** no funil de vendas. Em uma carteira de milhões, isso é a diferença entre bater a meta do ano ou não.\n",
        "\n",
        "**DECISÃO DE IMPLANTAÇÃO**\n",
        "\n",
        "Com base na evidência de que o **Gradient Boosting Otimizado** entrega:\n",
        "1.  Maior Receita Potencial (Maior Recall e F1);\n",
        "2.  Maior Segurança na Oferta (Maior ROC-AUC);\n",
        "3.  Viabilidade Técnica Total (Execução rápida após setup);\n",
        "\n",
        "...solicitamos a aprovação imediata para o **deploy deste modelo em produção**. O projeto está encerrado com sucesso."
      ],
      "metadata": {
        "id": "Xm45tTyrNx3J"
      },
      "id": "Xm45tTyrNx3J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RELATÓRIO DE ANÁLISE: SEGMENTAÇÃO DE CLIENTES DE ALTA RENDA\n",
        "\n",
        "Visão Geral e Objetivo de Negócio\n",
        "\n",
        "Este projeto teve como objetivo criar uma ferramenta que apoie diretamente a área comercial na identificação de clientes com renda anual acima de US$ 100 mil, com maior probabilidade de aderir a produtos financeiros de maior valor agregado (fundos exclusivos, previdência e serviços de gestão de patrimônio). Em termos práticos, o modelo permite que as equipes comerciais concentrem esforços nos clientes com maior potencial de aporte, reduzindo disparos genéricos e aumentando a taxa de conversão em carte de alta renda.\n",
        "\n",
        "Ao utilizar características de perfil (idade, escolaridade, ocupação, horas trabalhadas, histórico de capital, entre outras), o modelo identifica padrões associados a alta renda e transforma uma base estática de cadastros em uma lista priorizada de clientes, organizada por probabilidade de pertencer ao segmento-alvo.\n",
        "\n",
        "Tratamento de Dados e Responsabilidade no Uso de Informações\n",
        "\n",
        "Antes da modelagem, os dados passaram por um processo de limpeza, padronização e criação de variáveis agregadas (por exemplo, faixas etárias, grupos de escolaridade e tipos de vínculo profissional). Essas novas variáveis tornaram o modelo mais aderente à linguagem de negócio (faixas etárias, grupos de profissão) e mais eficiente na diferenciação entre clientes de maior e menor renda.\n",
        "\n",
        "Ao mesmo tempo, foi tomada uma decisão explícita de não utilizar variáveis sensíveis como raça, sexo e país de origem na etapa preditiva. Essa escolha está alinhada com boas práticas internacionais e com legislações como a LGPD no Brasil e o GDPR na União Europeia, que desestimulam o uso de atributos sensíveis em decisões automatizadas que possam gerar discriminação. Testes internos mostraram que a retirada desses campos praticamente não reduziu o poder de previsão, uma vez que as variáveis econômicas e de emprego já carregam a maior parte das informações relevantes para o objetivo de negócio. Dessa forma, o modelo mantém bom desempenho, respeitando limites éticos e regulatórios.\n",
        "\n",
        "Comparação dos Modelos Testados\n",
        "\n",
        "Foram avaliadas diferentes abordagens de modelagem, desde alternativas mais simples e rápidas até modelos mais sofisticados. Entre elas, destacam-se Regressão Logística, KNN, Árvore de Decisão, Random Forest e Gradient Boosting, em versões básica e otimizada.\n",
        "\n",
        "A principal conclusão é que há um equilíbrio claro entre três dimensões:\n",
        "\n",
        "– Quantos clientes de alta renda o modelo consegue encontrar (recall);\n",
        "\n",
        "– Qual a qualidade desses leads (equilíbrio entre acertos e erros – F1 e ROC-AUC);\n",
        "\n",
        "– Quanto custa, em termos de processamento, treinar e manter cada solução.\n",
        "\n",
        "Modelos mais simples, como a Regressão Logística, embora rápidos, deixam de identificar uma parcela relevante de clientes de alta renda, o que representa perda de oportunidade comercial. Já soluções mais pesadas, como algumas configurações de SVC, mostraram custo computacional elevado sem ganho proporcional em resultado, o que as torna pouco atrativas para uso em larga escala.\n",
        "\n",
        "Entre os modelos avaliados, o Gradient Boosting Otimizado apresentou o melhor equilíbrio global:\n",
        "– Maior capacidade de encontrar clientes de alta renda (maior recall);\n",
        "– Melhor combinação entre quantidade e qualidade dos leads (melhor F1-Score);\n",
        "– Maior capacidade de distinguir com clareza clientes de alta e baixa renda (maior ROC-AUC).\n",
        "\n",
        "Modelos de árvore mais simples, como a Decision Tree, mostraram-se extremamente rápidos e com desempenho competitivo, o que os torna úteis como referência e alternativa leve, mas ainda abaixo do patamar entregue pelo Gradient Boosting Otimizado.\n",
        "\n",
        "| Modelo                            | Acurácia | Precisão | Recall | F1-Score | ROC-AUC | Tempo (s) |\n",
        "|-----------------------------------|----------|----------|--------|----------|---------|-----------|\n",
        "| Gradient Boosting (Otimizado)     | 0.8633   | 0.7570   | 0.6602 | 0.7053   | 0.9212  | 98.34     |\n",
        "| Decision Tree (Original)          | 0.8480   | 0.7088   | 0.6555 | 0.6811   | 0.8984  | 0.04      |\n",
        "| Gradient Boosting (Default)       | 0.8571   | 0.7704   | 0.6029 | 0.6764   | 0.9124  | 1.76      |\n",
        "| Random Forest (Original)          | 0.8521   | 0.7972   | 0.5403 | 0.6441   | 0.9161  | 46.27     |\n",
        "| KNN (Original)                    | 0.8312   | 0.6874   | 0.5837 | 0.6314   | 0.8796  | 45.60     |\n",
        "| Decision Tree (New)               | 0.8404   | 0.7441   | 0.5420 | 0.6272   | 0.8874  | 0.05      |\n",
        "| KNN (New)                         | 0.8254   | 0.6750   | 0.5687 | 0.6173   | 0.8599  | 22.37     |\n",
        "| Logistic Regression (Original)    | 0.8184   | 0.7279   | 0.4257 | 0.5372   | 0.8499  | 3.24      |\n",
        "| Logistic Regression (New)         | 0.8027   | 0.6982   | 0.3578 | 0.4731   | 0.8169  | 0.53      |\n",
        "\n",
        "\n",
        "\n",
        "Escolha do Modelo para o Negócio\n",
        "\n",
        "Considerando a estratégia comercial da instituição, o maior risco é deixar clientes de alta renda fora do radar. Nessa lógica, é preferível abordar alguns clientes que não se confirmem como alta renda (erro de excesso) do que ignorar potenciais investidores relevantes (erro de omissão). O Gradient Boosting Otimizado é o que melhor atende a essa prioridade: identifica o maior número de clientes de alta renda, mantendo boa qualidade nos leads sugeridos à equipe comercial.\n",
        "\n",
        "Embora o tempo de treinamento deste modelo seja maior na fase de desenvolvimento, isso não afeta a operação do dia a dia, pois o tempo para avaliar novos clientes é baixo e compatível com uso em ambiente de produção, inclusive em bases amplas. As variáveis derivadas criadas no projeto também se mostraram essenciais para esse resultado, ao traduzir informações brutas em indicadores mais próximos da realidade de negócios.\n",
        "\n",
        "Conclusão e Próximos Passos\n",
        "\n",
        "O modelo final selecionado, baseado em Gradient Boosting Otimizado, oferece ao banco uma ferramenta concreta para ampliar a captação de clientes de alta renda, aumentar a eficiência das ações comerciais e fortalecer a estratégia de uso de dados no relacionamento com investidores. Ele permite priorizar esforços, reduzir abordagens pouco relevantes e direcionar produtos adequados ao potencial financeiro de cada cliente.\n",
        "\n",
        "Recomenda-se avançar com a implantação deste modelo nos sistemas internos da área comercial, com acompanhamento contínuo de desempenho e ajustes periódicos. A coleta de feedback das equipes de atendimento e vendas será fundamental para refinar o uso dos scores gerados, calibrar limiares de corte e, se necessário, ajustar o modelo em futuras versões.\n",
        "\n",
        "Esse projeto representa um passo concreto na direção de uma tomada de decisão mais orientada por dados, com impacto direto em receita, eficiência operacional e qualidade da experiência do cliente."
      ],
      "metadata": {
        "id": "OfTyVy7vDiZv"
      },
      "id": "OfTyVy7vDiZv"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}